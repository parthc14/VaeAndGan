{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gans.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDdbgJ_Amk4q"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Reshape\n",
        "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from keras.layers import LeakyReLU, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nGAmTKsmnJS"
      },
      "source": [
        "latent_dim = 100\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "img_channels = 1\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rfuZ-hsmqbz"
      },
      "source": [
        "def generator_model(): \n",
        "    dropout = 0.4\n",
        "    depth = 256 # 64+64+64+64\n",
        "    dim = 7\n",
        "    \n",
        "    model = Sequential()\n",
        "    # In: 100\n",
        "    # Out: dim x dim x depth\n",
        "    model.add(Dense(dim*dim*depth, input_dim=latent_dim))\n",
        "    model.add(BatchNormalization(momentum=0.9))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Reshape((dim, dim, depth)))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    # In: dim x dim x depth\n",
        "    # Out: 2*dim x 2*dim x depth/2\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.9))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.9))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.9))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
        "    model.add(Conv2DTranspose(1, 5, padding='same'))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me6PdV52mtH3"
      },
      "source": [
        "def discriminator_model():\n",
        "    depth = 64\n",
        "    dropout = 0.4\n",
        "    input_shape = (img_rows, img_cols, img_channels)\n",
        "    \n",
        "    model = Sequential()\n",
        "    # In: 28 x 28 x 1, depth = 1\n",
        "    # Out: 14 x 14 x 1, depth=64\n",
        "    model.add(Conv2D(depth, 5, strides=2, input_shape=input_shape, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    # Out: 1-dim probability\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNTUygwzmxkN"
      },
      "source": [
        "discriminator = discriminator_model()\n",
        "discriminator.compile(loss='binary_crossentropy', \n",
        "                      optimizer=RMSprop(lr=0.0002, decay=6e-8), \n",
        "                      metrics=['accuracy'])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwEnmxOfmzQu"
      },
      "source": [
        "generator = generator_model()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrDcAGVtm2s1"
      },
      "source": [
        "def adversarial_model():\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    discriminator.trainable = False\n",
        "    model.add(discriminator)\n",
        "    model.compile(loss='binary_crossentropy', \n",
        "                  optimizer=RMSprop(lr=0.0001, decay=3e-8), \n",
        "                  metrics=['accuracy'])\n",
        "    discriminator.trainable = True\n",
        "    return model\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJvMNs9Sm4fY"
      },
      "source": [
        "adversarial = adversarial_model()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhIC3vHjm84e"
      },
      "source": [
        "def plot_images(saveToFile=False, fake=True, samples=16, noise=None, epoch=0):\n",
        "    filename = 'mnist.png'\n",
        "    if fake:\n",
        "        if noise is None:\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[samples, latent_dim])\n",
        "        else:\n",
        "            filename = \"mnist_%d.png\" % epoch\n",
        "        images = generator.predict(noise)\n",
        "    else:\n",
        "        i = np.random.randint(0, x_train.shape[0], samples)\n",
        "        images = x_train[i, :, :, :]\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        image = images[i, :, :, :]\n",
        "        image = np.reshape(image, [img_rows, img_cols])\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    if saveToFile:\n",
        "        plt.savefig(filename)\n",
        "        plt.close('all')\n",
        "    else:\n",
        "        plt.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODyfy1A7nAJZ"
      },
      "source": [
        "def train(train_epochs=2000, batch_size=256, save_interval=0):\n",
        "        noise_input = None\n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_dim])\n",
        "        for epoch in range(train_epochs):\n",
        "            \n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            \n",
        "            # select a random half of images\n",
        "            images_train = x_train[np.random.randint(0, x_train.shape[0], size=batch_size), :, :, :]\n",
        "            \n",
        "            # sample noise and generate a batch of new images\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])\n",
        "            images_fake = generator.predict(noise)\n",
        "            \n",
        "            # train the discriminator (real classified as ones and generated as zeros)\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            d_loss = discriminator.train_on_batch(x, y)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "            \n",
        "            # train the generator (wants discriminator to mistake images as real)\n",
        "            y = np.ones([batch_size, 1])\n",
        "            a_loss = adversarial.train_on_batch(noise, y)\n",
        "            \n",
        "            log_msg = \"%d: [D loss: %f, acc: %f]\" % (epoch, d_loss[0], d_loss[1])\n",
        "            log_msg = \"%s  [A loss: %f, acc: %f]\" % (log_msg, a_loss[0], a_loss[1])\n",
        "            print(log_msg)\n",
        "            if save_interval>0:\n",
        "                if (epoch+1)%save_interval==0:\n",
        "                    plot_images(saveToFile=True, samples=noise_input.shape[0],\n",
        "                                noise=noise_input, epoch=(epoch+1))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TYor_ESnDiS"
      },
      "source": [
        "\n",
        "class ElapsedTimer(object):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    def elapsed(self,sec):\n",
        "        if sec < 60:\n",
        "            return str(sec) + \" sec\"\n",
        "        elif sec < (60 * 60):\n",
        "            return str(sec / 60) + \" min\"\n",
        "        else:\n",
        "            return str(sec / (60 * 60)) + \" hr\"\n",
        "    def elapsed_time(self):\n",
        "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vBn6EVpznGF0",
        "outputId": "fc8c329c-bbee-4cd2-e032-56691e67de24"
      },
      "source": [
        "timer = ElapsedTimer()\n",
        "train(train_epochs=1000, batch_size=256, save_interval=100) \n",
        "timer.elapsed_time()\n",
        "plot_images(fake=True)\n",
        "plot_images(fake=False, saveToFile=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: [D loss: 0.693339, acc: 0.472656]  [A loss: 1.286165, acc: 0.000000]\n",
            "1: [D loss: 0.580508, acc: 0.501953]  [A loss: 2.551824, acc: 0.000000]\n",
            "2: [D loss: 0.327937, acc: 0.949219]  [A loss: 0.019955, acc: 1.000000]\n",
            "3: [D loss: 1.462138, acc: 0.500000]  [A loss: 4.651537, acc: 0.000000]\n",
            "4: [D loss: 0.321909, acc: 0.873047]  [A loss: 1.956855, acc: 0.000000]\n",
            "5: [D loss: 0.180658, acc: 0.994141]  [A loss: 1.628862, acc: 0.000000]\n",
            "6: [D loss: 0.150455, acc: 0.990234]  [A loss: 1.276644, acc: 0.031250]\n",
            "7: [D loss: 0.125339, acc: 0.990234]  [A loss: 0.985269, acc: 0.226562]\n",
            "8: [D loss: 0.108587, acc: 0.988281]  [A loss: 0.731272, acc: 0.523438]\n",
            "9: [D loss: 0.092847, acc: 0.994141]  [A loss: 0.689439, acc: 0.539062]\n",
            "10: [D loss: 0.088709, acc: 0.992188]  [A loss: 0.493826, acc: 0.800781]\n",
            "11: [D loss: 0.082730, acc: 0.992188]  [A loss: 0.437852, acc: 0.843750]\n",
            "12: [D loss: 0.061978, acc: 1.000000]  [A loss: 0.685356, acc: 0.636719]\n",
            "13: [D loss: 0.057237, acc: 0.998047]  [A loss: 0.575390, acc: 0.683594]\n",
            "14: [D loss: 0.054588, acc: 0.992188]  [A loss: 0.439237, acc: 0.812500]\n",
            "15: [D loss: 0.044026, acc: 0.996094]  [A loss: 0.815448, acc: 0.480469]\n",
            "16: [D loss: 0.034251, acc: 0.996094]  [A loss: 0.479185, acc: 0.753906]\n",
            "17: [D loss: 0.031966, acc: 0.996094]  [A loss: 0.801822, acc: 0.507812]\n",
            "18: [D loss: 0.022958, acc: 0.998047]  [A loss: 0.169508, acc: 0.980469]\n",
            "19: [D loss: 0.023364, acc: 1.000000]  [A loss: 1.552057, acc: 0.171875]\n",
            "20: [D loss: 0.023934, acc: 0.994141]  [A loss: 0.004318, acc: 1.000000]\n",
            "21: [D loss: 0.027815, acc: 1.000000]  [A loss: 2.080056, acc: 0.066406]\n",
            "22: [D loss: 0.014640, acc: 1.000000]  [A loss: 0.582847, acc: 0.718750]\n",
            "23: [D loss: 0.007940, acc: 1.000000]  [A loss: 0.868612, acc: 0.535156]\n",
            "24: [D loss: 0.009761, acc: 1.000000]  [A loss: 0.877867, acc: 0.480469]\n",
            "25: [D loss: 0.011954, acc: 0.996094]  [A loss: 0.060754, acc: 1.000000]\n",
            "26: [D loss: 0.011255, acc: 1.000000]  [A loss: 2.084972, acc: 0.109375]\n",
            "27: [D loss: 0.004942, acc: 1.000000]  [A loss: 1.849398, acc: 0.132812]\n",
            "28: [D loss: 0.003787, acc: 1.000000]  [A loss: 0.933214, acc: 0.511719]\n",
            "29: [D loss: 0.010351, acc: 0.998047]  [A loss: 0.611836, acc: 0.683594]\n",
            "30: [D loss: 0.007000, acc: 1.000000]  [A loss: 2.151738, acc: 0.113281]\n",
            "31: [D loss: 0.003822, acc: 1.000000]  [A loss: 1.079566, acc: 0.398438]\n",
            "32: [D loss: 0.004041, acc: 1.000000]  [A loss: 3.295748, acc: 0.023438]\n",
            "33: [D loss: 0.004168, acc: 1.000000]  [A loss: 0.585882, acc: 0.691406]\n",
            "34: [D loss: 0.006468, acc: 1.000000]  [A loss: 7.129272, acc: 0.000000]\n",
            "35: [D loss: 0.019150, acc: 0.996094]  [A loss: 0.000001, acc: 1.000000]\n",
            "36: [D loss: 1.741952, acc: 0.500000]  [A loss: 22.975164, acc: 0.000000]\n",
            "37: [D loss: 2.203184, acc: 0.537109]  [A loss: 0.793534, acc: 0.535156]\n",
            "38: [D loss: 0.009724, acc: 1.000000]  [A loss: 0.638584, acc: 0.621094]\n",
            "39: [D loss: 0.014146, acc: 1.000000]  [A loss: 0.632406, acc: 0.656250]\n",
            "40: [D loss: 0.018404, acc: 1.000000]  [A loss: 0.951158, acc: 0.332031]\n",
            "41: [D loss: 0.020635, acc: 1.000000]  [A loss: 1.356894, acc: 0.210938]\n",
            "42: [D loss: 0.019684, acc: 1.000000]  [A loss: 1.934544, acc: 0.031250]\n",
            "43: [D loss: 0.018390, acc: 1.000000]  [A loss: 2.454762, acc: 0.003906]\n",
            "44: [D loss: 0.018652, acc: 1.000000]  [A loss: 2.927231, acc: 0.000000]\n",
            "45: [D loss: 0.014747, acc: 1.000000]  [A loss: 3.354851, acc: 0.000000]\n",
            "46: [D loss: 0.014631, acc: 1.000000]  [A loss: 3.647230, acc: 0.000000]\n",
            "47: [D loss: 0.015671, acc: 1.000000]  [A loss: 3.618121, acc: 0.000000]\n",
            "48: [D loss: 0.014813, acc: 1.000000]  [A loss: 3.904643, acc: 0.000000]\n",
            "49: [D loss: 0.020692, acc: 1.000000]  [A loss: 4.025308, acc: 0.000000]\n",
            "50: [D loss: 0.025184, acc: 0.996094]  [A loss: 3.258832, acc: 0.000000]\n",
            "51: [D loss: 0.027957, acc: 1.000000]  [A loss: 3.578802, acc: 0.003906]\n",
            "52: [D loss: 0.027233, acc: 0.996094]  [A loss: 3.037282, acc: 0.000000]\n",
            "53: [D loss: 0.029919, acc: 1.000000]  [A loss: 3.588851, acc: 0.003906]\n",
            "54: [D loss: 0.030178, acc: 0.996094]  [A loss: 2.150622, acc: 0.058594]\n",
            "55: [D loss: 0.042758, acc: 0.996094]  [A loss: 3.048444, acc: 0.019531]\n",
            "56: [D loss: 0.046913, acc: 0.988281]  [A loss: 1.444452, acc: 0.277344]\n",
            "57: [D loss: 0.052338, acc: 0.990234]  [A loss: 3.207379, acc: 0.011719]\n",
            "58: [D loss: 0.045319, acc: 0.986328]  [A loss: 0.644130, acc: 0.644531]\n",
            "59: [D loss: 0.098177, acc: 0.978516]  [A loss: 8.279875, acc: 0.000000]\n",
            "60: [D loss: 0.751815, acc: 0.757812]  [A loss: 0.002669, acc: 1.000000]\n",
            "61: [D loss: 2.074227, acc: 0.500000]  [A loss: 2.168900, acc: 0.019531]\n",
            "62: [D loss: 0.064863, acc: 0.990234]  [A loss: 1.505792, acc: 0.136719]\n",
            "63: [D loss: 0.059843, acc: 0.996094]  [A loss: 1.728446, acc: 0.062500]\n",
            "64: [D loss: 0.094219, acc: 0.988281]  [A loss: 1.576707, acc: 0.140625]\n",
            "65: [D loss: 0.084276, acc: 0.990234]  [A loss: 1.555364, acc: 0.117188]\n",
            "66: [D loss: 0.062275, acc: 0.994141]  [A loss: 1.680513, acc: 0.101562]\n",
            "67: [D loss: 0.060538, acc: 0.992188]  [A loss: 1.276205, acc: 0.226562]\n",
            "68: [D loss: 0.062746, acc: 0.996094]  [A loss: 1.835685, acc: 0.066406]\n",
            "69: [D loss: 0.078720, acc: 0.984375]  [A loss: 1.000433, acc: 0.390625]\n",
            "70: [D loss: 0.072312, acc: 0.992188]  [A loss: 2.386488, acc: 0.015625]\n",
            "71: [D loss: 0.104931, acc: 0.976562]  [A loss: 0.500473, acc: 0.746094]\n",
            "72: [D loss: 0.238001, acc: 0.894531]  [A loss: 7.605971, acc: 0.000000]\n",
            "73: [D loss: 1.652994, acc: 0.628906]  [A loss: 0.085785, acc: 1.000000]\n",
            "74: [D loss: 1.186438, acc: 0.500000]  [A loss: 1.232161, acc: 0.105469]\n",
            "75: [D loss: 0.147643, acc: 0.990234]  [A loss: 1.956548, acc: 0.003906]\n",
            "76: [D loss: 0.127861, acc: 0.980469]  [A loss: 1.590940, acc: 0.023438]\n",
            "77: [D loss: 0.126939, acc: 0.982422]  [A loss: 1.819810, acc: 0.003906]\n",
            "78: [D loss: 0.147753, acc: 0.970703]  [A loss: 1.536920, acc: 0.039062]\n",
            "79: [D loss: 0.160562, acc: 0.978516]  [A loss: 1.987481, acc: 0.003906]\n",
            "80: [D loss: 0.165533, acc: 0.966797]  [A loss: 1.381420, acc: 0.082031]\n",
            "81: [D loss: 0.208609, acc: 0.955078]  [A loss: 2.876023, acc: 0.000000]\n",
            "82: [D loss: 0.249426, acc: 0.935547]  [A loss: 0.848599, acc: 0.421875]\n",
            "83: [D loss: 0.351893, acc: 0.796875]  [A loss: 4.572138, acc: 0.000000]\n",
            "84: [D loss: 0.810296, acc: 0.746094]  [A loss: 0.348679, acc: 0.917969]\n",
            "85: [D loss: 0.821434, acc: 0.513672]  [A loss: 2.265800, acc: 0.000000]\n",
            "86: [D loss: 0.305455, acc: 0.916016]  [A loss: 1.129429, acc: 0.132812]\n",
            "87: [D loss: 0.311555, acc: 0.873047]  [A loss: 2.430516, acc: 0.000000]\n",
            "88: [D loss: 0.293232, acc: 0.912109]  [A loss: 1.123672, acc: 0.136719]\n",
            "89: [D loss: 0.328172, acc: 0.865234]  [A loss: 2.363867, acc: 0.000000]\n",
            "90: [D loss: 0.348008, acc: 0.896484]  [A loss: 0.873516, acc: 0.339844]\n",
            "91: [D loss: 0.466302, acc: 0.658203]  [A loss: 3.236550, acc: 0.000000]\n",
            "92: [D loss: 0.575154, acc: 0.750000]  [A loss: 0.540551, acc: 0.753906]\n",
            "93: [D loss: 0.672927, acc: 0.519531]  [A loss: 2.154989, acc: 0.000000]\n",
            "94: [D loss: 0.340877, acc: 0.906250]  [A loss: 1.228188, acc: 0.050781]\n",
            "95: [D loss: 0.366196, acc: 0.847656]  [A loss: 2.074467, acc: 0.000000]\n",
            "96: [D loss: 0.351152, acc: 0.919922]  [A loss: 1.227883, acc: 0.062500]\n",
            "97: [D loss: 0.384990, acc: 0.820312]  [A loss: 2.467806, acc: 0.000000]\n",
            "98: [D loss: 0.385284, acc: 0.882812]  [A loss: 0.906155, acc: 0.277344]\n",
            "99: [D loss: 0.490386, acc: 0.638672]  [A loss: 2.782575, acc: 0.000000]\n",
            "100: [D loss: 0.555997, acc: 0.781250]  [A loss: 0.585986, acc: 0.691406]\n",
            "101: [D loss: 0.719981, acc: 0.505859]  [A loss: 2.104411, acc: 0.000000]\n",
            "102: [D loss: 0.394539, acc: 0.882812]  [A loss: 1.086473, acc: 0.125000]\n",
            "103: [D loss: 0.422714, acc: 0.759766]  [A loss: 2.210579, acc: 0.000000]\n",
            "104: [D loss: 0.388768, acc: 0.875000]  [A loss: 1.027244, acc: 0.144531]\n",
            "105: [D loss: 0.428365, acc: 0.763672]  [A loss: 2.310628, acc: 0.000000]\n",
            "106: [D loss: 0.417587, acc: 0.853516]  [A loss: 0.822205, acc: 0.371094]\n",
            "107: [D loss: 0.568896, acc: 0.583984]  [A loss: 2.511483, acc: 0.000000]\n",
            "108: [D loss: 0.513677, acc: 0.771484]  [A loss: 0.635634, acc: 0.605469]\n",
            "109: [D loss: 0.603022, acc: 0.552734]  [A loss: 1.984997, acc: 0.000000]\n",
            "110: [D loss: 0.468536, acc: 0.822266]  [A loss: 0.795139, acc: 0.390625]\n",
            "111: [D loss: 0.523656, acc: 0.630859]  [A loss: 2.115216, acc: 0.000000]\n",
            "112: [D loss: 0.453991, acc: 0.808594]  [A loss: 0.797052, acc: 0.367188]\n",
            "113: [D loss: 0.521253, acc: 0.632812]  [A loss: 2.209661, acc: 0.000000]\n",
            "114: [D loss: 0.463655, acc: 0.822266]  [A loss: 0.754525, acc: 0.457031]\n",
            "115: [D loss: 0.520614, acc: 0.609375]  [A loss: 2.112341, acc: 0.000000]\n",
            "116: [D loss: 0.429831, acc: 0.835938]  [A loss: 0.885838, acc: 0.285156]\n",
            "117: [D loss: 0.456430, acc: 0.703125]  [A loss: 2.172544, acc: 0.000000]\n",
            "118: [D loss: 0.459358, acc: 0.824219]  [A loss: 0.749609, acc: 0.468750]\n",
            "119: [D loss: 0.526988, acc: 0.617188]  [A loss: 2.285761, acc: 0.000000]\n",
            "120: [D loss: 0.520797, acc: 0.771484]  [A loss: 0.646790, acc: 0.605469]\n",
            "121: [D loss: 0.576454, acc: 0.587891]  [A loss: 1.957633, acc: 0.000000]\n",
            "122: [D loss: 0.438293, acc: 0.845703]  [A loss: 0.899233, acc: 0.253906]\n",
            "123: [D loss: 0.453705, acc: 0.746094]  [A loss: 1.980008, acc: 0.000000]\n",
            "124: [D loss: 0.448937, acc: 0.828125]  [A loss: 0.769659, acc: 0.457031]\n",
            "125: [D loss: 0.511770, acc: 0.658203]  [A loss: 2.180317, acc: 0.000000]\n",
            "126: [D loss: 0.495272, acc: 0.785156]  [A loss: 0.625692, acc: 0.660156]\n",
            "127: [D loss: 0.572592, acc: 0.568359]  [A loss: 2.110223, acc: 0.000000]\n",
            "128: [D loss: 0.492676, acc: 0.771484]  [A loss: 0.731635, acc: 0.464844]\n",
            "129: [D loss: 0.519391, acc: 0.638672]  [A loss: 1.928267, acc: 0.000000]\n",
            "130: [D loss: 0.482271, acc: 0.792969]  [A loss: 0.756001, acc: 0.453125]\n",
            "131: [D loss: 0.477479, acc: 0.699219]  [A loss: 1.924928, acc: 0.000000]\n",
            "132: [D loss: 0.457948, acc: 0.820312]  [A loss: 0.729236, acc: 0.484375]\n",
            "133: [D loss: 0.468855, acc: 0.710938]  [A loss: 1.978257, acc: 0.000000]\n",
            "134: [D loss: 0.461651, acc: 0.792969]  [A loss: 0.669859, acc: 0.589844]\n",
            "135: [D loss: 0.511172, acc: 0.638672]  [A loss: 2.039670, acc: 0.000000]\n",
            "136: [D loss: 0.512526, acc: 0.773438]  [A loss: 0.647941, acc: 0.609375]\n",
            "137: [D loss: 0.500064, acc: 0.621094]  [A loss: 1.872897, acc: 0.000000]\n",
            "138: [D loss: 0.477501, acc: 0.781250]  [A loss: 0.699787, acc: 0.539062]\n",
            "139: [D loss: 0.472435, acc: 0.681641]  [A loss: 1.879535, acc: 0.000000]\n",
            "140: [D loss: 0.497841, acc: 0.802734]  [A loss: 0.588840, acc: 0.707031]\n",
            "141: [D loss: 0.510490, acc: 0.628906]  [A loss: 1.907821, acc: 0.000000]\n",
            "142: [D loss: 0.498984, acc: 0.738281]  [A loss: 0.652119, acc: 0.593750]\n",
            "143: [D loss: 0.504224, acc: 0.632812]  [A loss: 1.742801, acc: 0.000000]\n",
            "144: [D loss: 0.471664, acc: 0.804688]  [A loss: 0.732855, acc: 0.449219]\n",
            "145: [D loss: 0.487097, acc: 0.652344]  [A loss: 1.749977, acc: 0.000000]\n",
            "146: [D loss: 0.503835, acc: 0.755859]  [A loss: 0.609558, acc: 0.667969]\n",
            "147: [D loss: 0.552257, acc: 0.587891]  [A loss: 1.837052, acc: 0.000000]\n",
            "148: [D loss: 0.565240, acc: 0.691406]  [A loss: 0.615776, acc: 0.640625]\n",
            "149: [D loss: 0.537675, acc: 0.589844]  [A loss: 1.451190, acc: 0.003906]\n",
            "150: [D loss: 0.472392, acc: 0.832031]  [A loss: 0.821315, acc: 0.300781]\n",
            "151: [D loss: 0.456485, acc: 0.777344]  [A loss: 1.432832, acc: 0.000000]\n",
            "152: [D loss: 0.436779, acc: 0.871094]  [A loss: 0.768739, acc: 0.402344]\n",
            "153: [D loss: 0.452581, acc: 0.746094]  [A loss: 1.850192, acc: 0.000000]\n",
            "154: [D loss: 0.500509, acc: 0.751953]  [A loss: 0.501952, acc: 0.816406]\n",
            "155: [D loss: 0.552206, acc: 0.580078]  [A loss: 1.898543, acc: 0.000000]\n",
            "156: [D loss: 0.502031, acc: 0.710938]  [A loss: 0.733671, acc: 0.476562]\n",
            "157: [D loss: 0.490610, acc: 0.666016]  [A loss: 1.546313, acc: 0.000000]\n",
            "158: [D loss: 0.406711, acc: 0.886719]  [A loss: 0.952052, acc: 0.187500]\n",
            "159: [D loss: 0.417350, acc: 0.843750]  [A loss: 1.553371, acc: 0.007812]\n",
            "160: [D loss: 0.408122, acc: 0.875000]  [A loss: 0.800559, acc: 0.382812]\n",
            "161: [D loss: 0.478316, acc: 0.707031]  [A loss: 2.232498, acc: 0.000000]\n",
            "162: [D loss: 0.633397, acc: 0.605469]  [A loss: 0.421842, acc: 0.886719]\n",
            "163: [D loss: 0.726239, acc: 0.513672]  [A loss: 1.698620, acc: 0.000000]\n",
            "164: [D loss: 0.544424, acc: 0.708984]  [A loss: 0.789946, acc: 0.367188]\n",
            "165: [D loss: 0.507290, acc: 0.724609]  [A loss: 1.348885, acc: 0.007812]\n",
            "166: [D loss: 0.450866, acc: 0.867188]  [A loss: 0.965848, acc: 0.175781]\n",
            "167: [D loss: 0.453472, acc: 0.828125]  [A loss: 1.496870, acc: 0.003906]\n",
            "168: [D loss: 0.454892, acc: 0.822266]  [A loss: 0.656526, acc: 0.605469]\n",
            "169: [D loss: 0.521028, acc: 0.671875]  [A loss: 2.257396, acc: 0.000000]\n",
            "170: [D loss: 0.713374, acc: 0.554688]  [A loss: 0.483652, acc: 0.890625]\n",
            "171: [D loss: 0.645885, acc: 0.533203]  [A loss: 1.516090, acc: 0.003906]\n",
            "172: [D loss: 0.518309, acc: 0.761719]  [A loss: 0.879240, acc: 0.261719]\n",
            "173: [D loss: 0.466753, acc: 0.794922]  [A loss: 1.206972, acc: 0.015625]\n",
            "174: [D loss: 0.457455, acc: 0.841797]  [A loss: 0.982715, acc: 0.167969]\n",
            "175: [D loss: 0.423434, acc: 0.863281]  [A loss: 1.315520, acc: 0.031250]\n",
            "176: [D loss: 0.418872, acc: 0.878906]  [A loss: 0.855640, acc: 0.324219]\n",
            "177: [D loss: 0.451237, acc: 0.816406]  [A loss: 1.657459, acc: 0.000000]\n",
            "178: [D loss: 0.489103, acc: 0.769531]  [A loss: 0.425790, acc: 0.875000]\n",
            "179: [D loss: 0.684422, acc: 0.533203]  [A loss: 2.252129, acc: 0.000000]\n",
            "180: [D loss: 0.737494, acc: 0.527344]  [A loss: 0.646816, acc: 0.628906]\n",
            "181: [D loss: 0.585157, acc: 0.599609]  [A loss: 1.266405, acc: 0.035156]\n",
            "182: [D loss: 0.465930, acc: 0.841797]  [A loss: 1.019524, acc: 0.140625]\n",
            "183: [D loss: 0.423674, acc: 0.888672]  [A loss: 1.252558, acc: 0.058594]\n",
            "184: [D loss: 0.415413, acc: 0.873047]  [A loss: 0.980995, acc: 0.222656]\n",
            "185: [D loss: 0.423934, acc: 0.861328]  [A loss: 1.513592, acc: 0.023438]\n",
            "186: [D loss: 0.435237, acc: 0.841797]  [A loss: 0.564014, acc: 0.714844]\n",
            "187: [D loss: 0.573632, acc: 0.615234]  [A loss: 2.341807, acc: 0.000000]\n",
            "188: [D loss: 0.774647, acc: 0.527344]  [A loss: 0.599589, acc: 0.691406]\n",
            "189: [D loss: 0.592200, acc: 0.599609]  [A loss: 1.494292, acc: 0.007812]\n",
            "190: [D loss: 0.506846, acc: 0.779297]  [A loss: 0.910974, acc: 0.234375]\n",
            "191: [D loss: 0.469828, acc: 0.804688]  [A loss: 1.375797, acc: 0.023438]\n",
            "192: [D loss: 0.442879, acc: 0.851562]  [A loss: 0.883526, acc: 0.300781]\n",
            "193: [D loss: 0.438354, acc: 0.804688]  [A loss: 1.578353, acc: 0.003906]\n",
            "194: [D loss: 0.464664, acc: 0.783203]  [A loss: 0.570527, acc: 0.707031]\n",
            "195: [D loss: 0.569143, acc: 0.623047]  [A loss: 2.067368, acc: 0.000000]\n",
            "196: [D loss: 0.659112, acc: 0.585938]  [A loss: 0.659733, acc: 0.597656]\n",
            "197: [D loss: 0.590542, acc: 0.607422]  [A loss: 1.603478, acc: 0.003906]\n",
            "198: [D loss: 0.498346, acc: 0.763672]  [A loss: 0.838597, acc: 0.339844]\n",
            "199: [D loss: 0.468324, acc: 0.785156]  [A loss: 1.401564, acc: 0.027344]\n",
            "200: [D loss: 0.483049, acc: 0.787109]  [A loss: 0.765278, acc: 0.453125]\n",
            "201: [D loss: 0.494983, acc: 0.728516]  [A loss: 1.705893, acc: 0.000000]\n",
            "202: [D loss: 0.557747, acc: 0.689453]  [A loss: 0.595186, acc: 0.675781]\n",
            "203: [D loss: 0.598742, acc: 0.617188]  [A loss: 1.830435, acc: 0.003906]\n",
            "204: [D loss: 0.632496, acc: 0.607422]  [A loss: 0.663846, acc: 0.542969]\n",
            "205: [D loss: 0.565093, acc: 0.648438]  [A loss: 1.333031, acc: 0.019531]\n",
            "206: [D loss: 0.582879, acc: 0.699219]  [A loss: 0.773335, acc: 0.425781]\n",
            "207: [D loss: 0.529983, acc: 0.708984]  [A loss: 1.396775, acc: 0.019531]\n",
            "208: [D loss: 0.473118, acc: 0.812500]  [A loss: 0.777795, acc: 0.453125]\n",
            "209: [D loss: 0.464226, acc: 0.757812]  [A loss: 1.729874, acc: 0.003906]\n",
            "210: [D loss: 0.526222, acc: 0.734375]  [A loss: 0.485266, acc: 0.816406]\n",
            "211: [D loss: 0.608829, acc: 0.611328]  [A loss: 2.069020, acc: 0.000000]\n",
            "212: [D loss: 0.702659, acc: 0.550781]  [A loss: 0.596670, acc: 0.671875]\n",
            "213: [D loss: 0.557392, acc: 0.636719]  [A loss: 1.417743, acc: 0.027344]\n",
            "214: [D loss: 0.542458, acc: 0.740234]  [A loss: 0.779994, acc: 0.449219]\n",
            "215: [D loss: 0.517044, acc: 0.736328]  [A loss: 1.385104, acc: 0.023438]\n",
            "216: [D loss: 0.536354, acc: 0.750000]  [A loss: 0.722763, acc: 0.496094]\n",
            "217: [D loss: 0.537558, acc: 0.695312]  [A loss: 1.452235, acc: 0.015625]\n",
            "218: [D loss: 0.555146, acc: 0.693359]  [A loss: 0.642178, acc: 0.613281]\n",
            "219: [D loss: 0.565223, acc: 0.669922]  [A loss: 1.591210, acc: 0.011719]\n",
            "220: [D loss: 0.594253, acc: 0.650391]  [A loss: 0.536240, acc: 0.738281]\n",
            "221: [D loss: 0.609951, acc: 0.619141]  [A loss: 1.639727, acc: 0.000000]\n",
            "222: [D loss: 0.629031, acc: 0.589844]  [A loss: 0.674164, acc: 0.554688]\n",
            "223: [D loss: 0.572761, acc: 0.640625]  [A loss: 1.332717, acc: 0.027344]\n",
            "224: [D loss: 0.534018, acc: 0.734375]  [A loss: 0.707122, acc: 0.507812]\n",
            "225: [D loss: 0.561243, acc: 0.679688]  [A loss: 1.502945, acc: 0.007812]\n",
            "226: [D loss: 0.594986, acc: 0.638672]  [A loss: 0.677166, acc: 0.574219]\n",
            "227: [D loss: 0.604948, acc: 0.630859]  [A loss: 1.441135, acc: 0.003906]\n",
            "228: [D loss: 0.569510, acc: 0.681641]  [A loss: 0.510206, acc: 0.750000]\n",
            "229: [D loss: 0.655382, acc: 0.617188]  [A loss: 1.763582, acc: 0.000000]\n",
            "230: [D loss: 0.716253, acc: 0.533203]  [A loss: 0.691214, acc: 0.578125]\n",
            "231: [D loss: 0.619390, acc: 0.601562]  [A loss: 1.268796, acc: 0.035156]\n",
            "232: [D loss: 0.574815, acc: 0.716797]  [A loss: 0.734937, acc: 0.488281]\n",
            "233: [D loss: 0.601358, acc: 0.626953]  [A loss: 1.297534, acc: 0.035156]\n",
            "234: [D loss: 0.579343, acc: 0.705078]  [A loss: 0.713851, acc: 0.535156]\n",
            "235: [D loss: 0.569032, acc: 0.673828]  [A loss: 1.348823, acc: 0.019531]\n",
            "236: [D loss: 0.597266, acc: 0.658203]  [A loss: 0.599901, acc: 0.667969]\n",
            "237: [D loss: 0.640804, acc: 0.611328]  [A loss: 1.578604, acc: 0.003906]\n",
            "238: [D loss: 0.689002, acc: 0.558594]  [A loss: 0.621796, acc: 0.683594]\n",
            "239: [D loss: 0.660063, acc: 0.574219]  [A loss: 1.251099, acc: 0.042969]\n",
            "240: [D loss: 0.571347, acc: 0.708984]  [A loss: 0.604683, acc: 0.656250]\n",
            "241: [D loss: 0.671136, acc: 0.605469]  [A loss: 1.547756, acc: 0.003906]\n",
            "242: [D loss: 0.654669, acc: 0.572266]  [A loss: 0.686364, acc: 0.546875]\n",
            "243: [D loss: 0.634864, acc: 0.601562]  [A loss: 1.201823, acc: 0.035156]\n",
            "244: [D loss: 0.600904, acc: 0.679688]  [A loss: 0.733388, acc: 0.449219]\n",
            "245: [D loss: 0.576428, acc: 0.701172]  [A loss: 1.272072, acc: 0.019531]\n",
            "246: [D loss: 0.593568, acc: 0.681641]  [A loss: 0.613288, acc: 0.656250]\n",
            "247: [D loss: 0.631121, acc: 0.611328]  [A loss: 1.422709, acc: 0.007812]\n",
            "248: [D loss: 0.661429, acc: 0.578125]  [A loss: 0.612560, acc: 0.644531]\n",
            "249: [D loss: 0.653496, acc: 0.558594]  [A loss: 1.316303, acc: 0.011719]\n",
            "250: [D loss: 0.616388, acc: 0.626953]  [A loss: 0.600713, acc: 0.667969]\n",
            "251: [D loss: 0.612883, acc: 0.625000]  [A loss: 1.365039, acc: 0.011719]\n",
            "252: [D loss: 0.631590, acc: 0.619141]  [A loss: 0.617591, acc: 0.632812]\n",
            "253: [D loss: 0.599141, acc: 0.632812]  [A loss: 1.344533, acc: 0.007812]\n",
            "254: [D loss: 0.627669, acc: 0.607422]  [A loss: 0.718368, acc: 0.468750]\n",
            "255: [D loss: 0.653881, acc: 0.583984]  [A loss: 1.241465, acc: 0.027344]\n",
            "256: [D loss: 0.607700, acc: 0.660156]  [A loss: 0.640938, acc: 0.640625]\n",
            "257: [D loss: 0.618569, acc: 0.601562]  [A loss: 1.413708, acc: 0.011719]\n",
            "258: [D loss: 0.671602, acc: 0.531250]  [A loss: 0.571901, acc: 0.718750]\n",
            "259: [D loss: 0.653659, acc: 0.564453]  [A loss: 1.345109, acc: 0.007812]\n",
            "260: [D loss: 0.655758, acc: 0.576172]  [A loss: 0.717630, acc: 0.503906]\n",
            "261: [D loss: 0.625565, acc: 0.626953]  [A loss: 1.089593, acc: 0.050781]\n",
            "262: [D loss: 0.615114, acc: 0.687500]  [A loss: 0.669204, acc: 0.558594]\n",
            "263: [D loss: 0.600765, acc: 0.652344]  [A loss: 1.173906, acc: 0.031250]\n",
            "264: [D loss: 0.613800, acc: 0.656250]  [A loss: 0.669459, acc: 0.582031]\n",
            "265: [D loss: 0.632239, acc: 0.609375]  [A loss: 1.259237, acc: 0.007812]\n",
            "266: [D loss: 0.644924, acc: 0.599609]  [A loss: 0.581213, acc: 0.722656]\n",
            "267: [D loss: 0.686215, acc: 0.533203]  [A loss: 1.381627, acc: 0.007812]\n",
            "268: [D loss: 0.698354, acc: 0.529297]  [A loss: 0.620321, acc: 0.683594]\n",
            "269: [D loss: 0.640457, acc: 0.587891]  [A loss: 1.086356, acc: 0.062500]\n",
            "270: [D loss: 0.609746, acc: 0.662109]  [A loss: 0.702696, acc: 0.507812]\n",
            "271: [D loss: 0.618530, acc: 0.630859]  [A loss: 1.114887, acc: 0.027344]\n",
            "272: [D loss: 0.633616, acc: 0.619141]  [A loss: 0.657725, acc: 0.570312]\n",
            "273: [D loss: 0.634970, acc: 0.615234]  [A loss: 1.188068, acc: 0.015625]\n",
            "274: [D loss: 0.646073, acc: 0.589844]  [A loss: 0.600711, acc: 0.660156]\n",
            "275: [D loss: 0.648804, acc: 0.578125]  [A loss: 1.275491, acc: 0.011719]\n",
            "276: [D loss: 0.649041, acc: 0.578125]  [A loss: 0.609028, acc: 0.687500]\n",
            "277: [D loss: 0.639952, acc: 0.582031]  [A loss: 1.207702, acc: 0.023438]\n",
            "278: [D loss: 0.642729, acc: 0.595703]  [A loss: 0.609969, acc: 0.656250]\n",
            "279: [D loss: 0.644164, acc: 0.582031]  [A loss: 1.206280, acc: 0.019531]\n",
            "280: [D loss: 0.646956, acc: 0.583984]  [A loss: 0.693704, acc: 0.519531]\n",
            "281: [D loss: 0.635315, acc: 0.625000]  [A loss: 1.047078, acc: 0.066406]\n",
            "282: [D loss: 0.615243, acc: 0.689453]  [A loss: 0.754121, acc: 0.453125]\n",
            "283: [D loss: 0.608244, acc: 0.675781]  [A loss: 1.049142, acc: 0.082031]\n",
            "284: [D loss: 0.623416, acc: 0.679688]  [A loss: 0.684752, acc: 0.539062]\n",
            "285: [D loss: 0.603332, acc: 0.644531]  [A loss: 1.188310, acc: 0.015625]\n",
            "286: [D loss: 0.626072, acc: 0.623047]  [A loss: 0.595125, acc: 0.703125]\n",
            "287: [D loss: 0.649400, acc: 0.570312]  [A loss: 1.426796, acc: 0.000000]\n",
            "288: [D loss: 0.696306, acc: 0.539062]  [A loss: 0.489064, acc: 0.882812]\n",
            "289: [D loss: 0.684157, acc: 0.542969]  [A loss: 1.246916, acc: 0.011719]\n",
            "290: [D loss: 0.677928, acc: 0.544922]  [A loss: 0.707017, acc: 0.523438]\n",
            "291: [D loss: 0.624204, acc: 0.646484]  [A loss: 0.993833, acc: 0.105469]\n",
            "292: [D loss: 0.610950, acc: 0.677734]  [A loss: 0.765888, acc: 0.375000]\n",
            "293: [D loss: 0.606307, acc: 0.695312]  [A loss: 1.010176, acc: 0.101562]\n",
            "294: [D loss: 0.609057, acc: 0.667969]  [A loss: 0.756873, acc: 0.433594]\n",
            "295: [D loss: 0.608083, acc: 0.689453]  [A loss: 1.068302, acc: 0.058594]\n",
            "296: [D loss: 0.613253, acc: 0.660156]  [A loss: 0.574776, acc: 0.738281]\n",
            "297: [D loss: 0.655363, acc: 0.552734]  [A loss: 1.507630, acc: 0.003906]\n",
            "298: [D loss: 0.721166, acc: 0.521484]  [A loss: 0.547444, acc: 0.796875]\n",
            "299: [D loss: 0.702526, acc: 0.527344]  [A loss: 1.208753, acc: 0.011719]\n",
            "300: [D loss: 0.651887, acc: 0.576172]  [A loss: 0.706764, acc: 0.500000]\n",
            "301: [D loss: 0.634597, acc: 0.634766]  [A loss: 0.990027, acc: 0.082031]\n",
            "302: [D loss: 0.624997, acc: 0.646484]  [A loss: 0.705511, acc: 0.531250]\n",
            "303: [D loss: 0.612648, acc: 0.683594]  [A loss: 0.939661, acc: 0.148438]\n",
            "304: [D loss: 0.621894, acc: 0.662109]  [A loss: 0.794752, acc: 0.339844]\n",
            "305: [D loss: 0.620432, acc: 0.662109]  [A loss: 0.970033, acc: 0.121094]\n",
            "306: [D loss: 0.601186, acc: 0.708984]  [A loss: 0.752849, acc: 0.445312]\n",
            "307: [D loss: 0.615181, acc: 0.673828]  [A loss: 1.095534, acc: 0.039062]\n",
            "308: [D loss: 0.645078, acc: 0.603516]  [A loss: 0.570475, acc: 0.742188]\n",
            "309: [D loss: 0.680713, acc: 0.552734]  [A loss: 1.462665, acc: 0.000000]\n",
            "310: [D loss: 0.717972, acc: 0.513672]  [A loss: 0.560107, acc: 0.800781]\n",
            "311: [D loss: 0.671540, acc: 0.552734]  [A loss: 1.104494, acc: 0.031250]\n",
            "312: [D loss: 0.668841, acc: 0.578125]  [A loss: 0.701232, acc: 0.535156]\n",
            "313: [D loss: 0.627873, acc: 0.642578]  [A loss: 0.928006, acc: 0.125000]\n",
            "314: [D loss: 0.600818, acc: 0.718750]  [A loss: 0.723976, acc: 0.464844]\n",
            "315: [D loss: 0.624801, acc: 0.666016]  [A loss: 0.932266, acc: 0.191406]\n",
            "316: [D loss: 0.606090, acc: 0.716797]  [A loss: 0.755990, acc: 0.437500]\n",
            "317: [D loss: 0.633292, acc: 0.636719]  [A loss: 1.053005, acc: 0.058594]\n",
            "318: [D loss: 0.646193, acc: 0.609375]  [A loss: 0.601120, acc: 0.710938]\n",
            "319: [D loss: 0.647732, acc: 0.589844]  [A loss: 1.330482, acc: 0.000000]\n",
            "320: [D loss: 0.697901, acc: 0.529297]  [A loss: 0.567602, acc: 0.757812]\n",
            "321: [D loss: 0.666940, acc: 0.556641]  [A loss: 1.192230, acc: 0.015625]\n",
            "322: [D loss: 0.660933, acc: 0.580078]  [A loss: 0.692070, acc: 0.527344]\n",
            "323: [D loss: 0.630826, acc: 0.671875]  [A loss: 0.982053, acc: 0.085938]\n",
            "324: [D loss: 0.631148, acc: 0.656250]  [A loss: 0.699516, acc: 0.503906]\n",
            "325: [D loss: 0.640143, acc: 0.648438]  [A loss: 1.010262, acc: 0.062500]\n",
            "326: [D loss: 0.643839, acc: 0.613281]  [A loss: 0.675296, acc: 0.558594]\n",
            "327: [D loss: 0.618986, acc: 0.644531]  [A loss: 1.012329, acc: 0.082031]\n",
            "328: [D loss: 0.638348, acc: 0.625000]  [A loss: 0.676360, acc: 0.582031]\n",
            "329: [D loss: 0.628337, acc: 0.634766]  [A loss: 1.052612, acc: 0.050781]\n",
            "330: [D loss: 0.638000, acc: 0.630859]  [A loss: 0.573003, acc: 0.750000]\n",
            "331: [D loss: 0.654199, acc: 0.572266]  [A loss: 1.248037, acc: 0.000000]\n",
            "332: [D loss: 0.692708, acc: 0.542969]  [A loss: 0.634329, acc: 0.640625]\n",
            "333: [D loss: 0.659907, acc: 0.580078]  [A loss: 1.029185, acc: 0.054688]\n",
            "334: [D loss: 0.637226, acc: 0.623047]  [A loss: 0.707530, acc: 0.500000]\n",
            "335: [D loss: 0.623586, acc: 0.673828]  [A loss: 0.955017, acc: 0.140625]\n",
            "336: [D loss: 0.626955, acc: 0.652344]  [A loss: 0.754234, acc: 0.390625]\n",
            "337: [D loss: 0.612379, acc: 0.699219]  [A loss: 0.923934, acc: 0.148438]\n",
            "338: [D loss: 0.641077, acc: 0.625000]  [A loss: 0.717582, acc: 0.503906]\n",
            "339: [D loss: 0.626740, acc: 0.654297]  [A loss: 1.013619, acc: 0.101562]\n",
            "340: [D loss: 0.647433, acc: 0.621094]  [A loss: 0.674550, acc: 0.566406]\n",
            "341: [D loss: 0.632338, acc: 0.623047]  [A loss: 1.142254, acc: 0.062500]\n",
            "342: [D loss: 0.678377, acc: 0.583984]  [A loss: 0.545584, acc: 0.757812]\n",
            "343: [D loss: 0.684171, acc: 0.560547]  [A loss: 1.290681, acc: 0.003906]\n",
            "344: [D loss: 0.692271, acc: 0.533203]  [A loss: 0.689789, acc: 0.550781]\n",
            "345: [D loss: 0.676985, acc: 0.548828]  [A loss: 0.979872, acc: 0.113281]\n",
            "346: [D loss: 0.634583, acc: 0.658203]  [A loss: 0.713224, acc: 0.453125]\n",
            "347: [D loss: 0.637186, acc: 0.632812]  [A loss: 0.958892, acc: 0.113281]\n",
            "348: [D loss: 0.632718, acc: 0.632812]  [A loss: 0.667560, acc: 0.585938]\n",
            "349: [D loss: 0.634804, acc: 0.630859]  [A loss: 1.073828, acc: 0.042969]\n",
            "350: [D loss: 0.629039, acc: 0.625000]  [A loss: 0.723281, acc: 0.480469]\n",
            "351: [D loss: 0.655549, acc: 0.597656]  [A loss: 1.028261, acc: 0.085938]\n",
            "352: [D loss: 0.641995, acc: 0.626953]  [A loss: 0.622560, acc: 0.667969]\n",
            "353: [D loss: 0.677010, acc: 0.582031]  [A loss: 1.180847, acc: 0.015625]\n",
            "354: [D loss: 0.684622, acc: 0.541016]  [A loss: 0.664110, acc: 0.535156]\n",
            "355: [D loss: 0.638352, acc: 0.609375]  [A loss: 0.974986, acc: 0.109375]\n",
            "356: [D loss: 0.633336, acc: 0.662109]  [A loss: 0.711248, acc: 0.496094]\n",
            "357: [D loss: 0.651412, acc: 0.613281]  [A loss: 0.960901, acc: 0.125000]\n",
            "358: [D loss: 0.636459, acc: 0.644531]  [A loss: 0.763467, acc: 0.386719]\n",
            "359: [D loss: 0.630253, acc: 0.662109]  [A loss: 0.921973, acc: 0.175781]\n",
            "360: [D loss: 0.631573, acc: 0.654297]  [A loss: 0.716059, acc: 0.472656]\n",
            "361: [D loss: 0.657820, acc: 0.615234]  [A loss: 1.189461, acc: 0.027344]\n",
            "362: [D loss: 0.669437, acc: 0.576172]  [A loss: 0.581288, acc: 0.742188]\n",
            "363: [D loss: 0.661587, acc: 0.560547]  [A loss: 1.178400, acc: 0.023438]\n",
            "364: [D loss: 0.670758, acc: 0.558594]  [A loss: 0.675808, acc: 0.554688]\n",
            "365: [D loss: 0.632816, acc: 0.648438]  [A loss: 0.942455, acc: 0.136719]\n",
            "366: [D loss: 0.643732, acc: 0.634766]  [A loss: 0.710801, acc: 0.468750]\n",
            "367: [D loss: 0.633859, acc: 0.642578]  [A loss: 0.946305, acc: 0.125000]\n",
            "368: [D loss: 0.633956, acc: 0.646484]  [A loss: 0.711984, acc: 0.496094]\n",
            "369: [D loss: 0.631004, acc: 0.619141]  [A loss: 0.981605, acc: 0.117188]\n",
            "370: [D loss: 0.634652, acc: 0.652344]  [A loss: 0.676228, acc: 0.554688]\n",
            "371: [D loss: 0.647043, acc: 0.595703]  [A loss: 1.148363, acc: 0.023438]\n",
            "372: [D loss: 0.683162, acc: 0.568359]  [A loss: 0.523498, acc: 0.808594]\n",
            "373: [D loss: 0.676703, acc: 0.562500]  [A loss: 1.189617, acc: 0.027344]\n",
            "374: [D loss: 0.694174, acc: 0.531250]  [A loss: 0.726125, acc: 0.453125]\n",
            "375: [D loss: 0.655431, acc: 0.626953]  [A loss: 0.872466, acc: 0.164062]\n",
            "376: [D loss: 0.638129, acc: 0.650391]  [A loss: 0.785145, acc: 0.328125]\n",
            "377: [D loss: 0.638642, acc: 0.646484]  [A loss: 0.820069, acc: 0.281250]\n",
            "378: [D loss: 0.637387, acc: 0.675781]  [A loss: 0.836435, acc: 0.285156]\n",
            "379: [D loss: 0.628799, acc: 0.644531]  [A loss: 0.858174, acc: 0.257812]\n",
            "380: [D loss: 0.629011, acc: 0.673828]  [A loss: 0.836455, acc: 0.304688]\n",
            "381: [D loss: 0.632191, acc: 0.662109]  [A loss: 0.863004, acc: 0.250000]\n",
            "382: [D loss: 0.631397, acc: 0.644531]  [A loss: 0.861266, acc: 0.273438]\n",
            "383: [D loss: 0.628749, acc: 0.654297]  [A loss: 0.793397, acc: 0.367188]\n",
            "384: [D loss: 0.634507, acc: 0.644531]  [A loss: 1.068867, acc: 0.136719]\n",
            "385: [D loss: 0.660127, acc: 0.611328]  [A loss: 0.538488, acc: 0.808594]\n",
            "386: [D loss: 0.685026, acc: 0.564453]  [A loss: 1.426309, acc: 0.007812]\n",
            "387: [D loss: 0.763226, acc: 0.505859]  [A loss: 0.653786, acc: 0.609375]\n",
            "388: [D loss: 0.680265, acc: 0.554688]  [A loss: 0.933893, acc: 0.140625]\n",
            "389: [D loss: 0.629277, acc: 0.664062]  [A loss: 0.784762, acc: 0.324219]\n",
            "390: [D loss: 0.636717, acc: 0.660156]  [A loss: 0.844969, acc: 0.292969]\n",
            "391: [D loss: 0.635665, acc: 0.654297]  [A loss: 0.869404, acc: 0.207031]\n",
            "392: [D loss: 0.628496, acc: 0.654297]  [A loss: 0.769284, acc: 0.425781]\n",
            "393: [D loss: 0.626488, acc: 0.656250]  [A loss: 0.897521, acc: 0.234375]\n",
            "394: [D loss: 0.632000, acc: 0.648438]  [A loss: 0.728982, acc: 0.476562]\n",
            "395: [D loss: 0.628711, acc: 0.644531]  [A loss: 1.156967, acc: 0.039062]\n",
            "396: [D loss: 0.649823, acc: 0.597656]  [A loss: 0.572875, acc: 0.718750]\n",
            "397: [D loss: 0.687204, acc: 0.564453]  [A loss: 1.276787, acc: 0.003906]\n",
            "398: [D loss: 0.697260, acc: 0.542969]  [A loss: 0.695809, acc: 0.531250]\n",
            "399: [D loss: 0.631434, acc: 0.638672]  [A loss: 0.918636, acc: 0.160156]\n",
            "400: [D loss: 0.628886, acc: 0.689453]  [A loss: 0.723996, acc: 0.449219]\n",
            "401: [D loss: 0.641546, acc: 0.648438]  [A loss: 0.951326, acc: 0.156250]\n",
            "402: [D loss: 0.636182, acc: 0.650391]  [A loss: 0.734810, acc: 0.460938]\n",
            "403: [D loss: 0.642114, acc: 0.638672]  [A loss: 0.995030, acc: 0.089844]\n",
            "404: [D loss: 0.648123, acc: 0.621094]  [A loss: 0.700423, acc: 0.515625]\n",
            "405: [D loss: 0.633903, acc: 0.623047]  [A loss: 1.021816, acc: 0.070312]\n",
            "406: [D loss: 0.638471, acc: 0.619141]  [A loss: 0.681073, acc: 0.562500]\n",
            "407: [D loss: 0.632035, acc: 0.636719]  [A loss: 1.114082, acc: 0.035156]\n",
            "408: [D loss: 0.655762, acc: 0.603516]  [A loss: 0.694274, acc: 0.550781]\n",
            "409: [D loss: 0.646269, acc: 0.609375]  [A loss: 1.022136, acc: 0.074219]\n",
            "410: [D loss: 0.651217, acc: 0.623047]  [A loss: 0.681245, acc: 0.527344]\n",
            "411: [D loss: 0.651895, acc: 0.578125]  [A loss: 1.088968, acc: 0.050781]\n",
            "412: [D loss: 0.650214, acc: 0.611328]  [A loss: 0.691154, acc: 0.535156]\n",
            "413: [D loss: 0.640251, acc: 0.642578]  [A loss: 1.001758, acc: 0.089844]\n",
            "414: [D loss: 0.650298, acc: 0.603516]  [A loss: 0.679921, acc: 0.562500]\n",
            "415: [D loss: 0.634131, acc: 0.625000]  [A loss: 1.037843, acc: 0.085938]\n",
            "416: [D loss: 0.642869, acc: 0.640625]  [A loss: 0.669971, acc: 0.566406]\n",
            "417: [D loss: 0.641356, acc: 0.611328]  [A loss: 1.100124, acc: 0.039062]\n",
            "418: [D loss: 0.643822, acc: 0.595703]  [A loss: 0.698488, acc: 0.523438]\n",
            "419: [D loss: 0.629210, acc: 0.615234]  [A loss: 1.022207, acc: 0.085938]\n",
            "420: [D loss: 0.631252, acc: 0.640625]  [A loss: 0.698207, acc: 0.480469]\n",
            "421: [D loss: 0.649488, acc: 0.626953]  [A loss: 1.042551, acc: 0.105469]\n",
            "422: [D loss: 0.649825, acc: 0.597656]  [A loss: 0.666550, acc: 0.570312]\n",
            "423: [D loss: 0.637084, acc: 0.628906]  [A loss: 1.100138, acc: 0.058594]\n",
            "424: [D loss: 0.643289, acc: 0.605469]  [A loss: 0.715169, acc: 0.476562]\n",
            "425: [D loss: 0.633439, acc: 0.634766]  [A loss: 1.021517, acc: 0.085938]\n",
            "426: [D loss: 0.640796, acc: 0.626953]  [A loss: 0.733895, acc: 0.468750]\n",
            "427: [D loss: 0.643183, acc: 0.613281]  [A loss: 1.033753, acc: 0.117188]\n",
            "428: [D loss: 0.649414, acc: 0.626953]  [A loss: 0.671692, acc: 0.609375]\n",
            "429: [D loss: 0.633112, acc: 0.656250]  [A loss: 1.118534, acc: 0.050781]\n",
            "430: [D loss: 0.639670, acc: 0.599609]  [A loss: 0.610931, acc: 0.695312]\n",
            "431: [D loss: 0.659801, acc: 0.580078]  [A loss: 1.163111, acc: 0.042969]\n",
            "432: [D loss: 0.665681, acc: 0.568359]  [A loss: 0.696457, acc: 0.531250]\n",
            "433: [D loss: 0.637469, acc: 0.617188]  [A loss: 0.981529, acc: 0.125000]\n",
            "434: [D loss: 0.636268, acc: 0.632812]  [A loss: 0.728671, acc: 0.507812]\n",
            "435: [D loss: 0.627176, acc: 0.656250]  [A loss: 0.977334, acc: 0.132812]\n",
            "436: [D loss: 0.616783, acc: 0.673828]  [A loss: 0.725113, acc: 0.437500]\n",
            "437: [D loss: 0.627727, acc: 0.644531]  [A loss: 1.015627, acc: 0.085938]\n",
            "438: [D loss: 0.634830, acc: 0.630859]  [A loss: 0.695689, acc: 0.519531]\n",
            "439: [D loss: 0.639879, acc: 0.634766]  [A loss: 1.168314, acc: 0.042969]\n",
            "440: [D loss: 0.668378, acc: 0.580078]  [A loss: 0.645678, acc: 0.597656]\n",
            "441: [D loss: 0.647905, acc: 0.597656]  [A loss: 1.041942, acc: 0.093750]\n",
            "442: [D loss: 0.652224, acc: 0.591797]  [A loss: 0.726564, acc: 0.476562]\n",
            "443: [D loss: 0.632540, acc: 0.662109]  [A loss: 0.945414, acc: 0.132812]\n",
            "444: [D loss: 0.644658, acc: 0.636719]  [A loss: 0.764356, acc: 0.371094]\n",
            "445: [D loss: 0.633553, acc: 0.652344]  [A loss: 0.923053, acc: 0.218750]\n",
            "446: [D loss: 0.623832, acc: 0.675781]  [A loss: 0.740666, acc: 0.460938]\n",
            "447: [D loss: 0.654912, acc: 0.613281]  [A loss: 1.040802, acc: 0.097656]\n",
            "448: [D loss: 0.645146, acc: 0.630859]  [A loss: 0.685817, acc: 0.527344]\n",
            "449: [D loss: 0.651323, acc: 0.619141]  [A loss: 1.078011, acc: 0.070312]\n",
            "450: [D loss: 0.655025, acc: 0.609375]  [A loss: 0.709360, acc: 0.523438]\n",
            "451: [D loss: 0.633341, acc: 0.626953]  [A loss: 0.969903, acc: 0.121094]\n",
            "452: [D loss: 0.633733, acc: 0.660156]  [A loss: 0.740128, acc: 0.457031]\n",
            "453: [D loss: 0.642374, acc: 0.632812]  [A loss: 1.047826, acc: 0.109375]\n",
            "454: [D loss: 0.631878, acc: 0.662109]  [A loss: 0.701517, acc: 0.535156]\n",
            "455: [D loss: 0.646786, acc: 0.625000]  [A loss: 1.147279, acc: 0.023438]\n",
            "456: [D loss: 0.632861, acc: 0.626953]  [A loss: 0.656834, acc: 0.625000]\n",
            "457: [D loss: 0.640513, acc: 0.613281]  [A loss: 1.116076, acc: 0.058594]\n",
            "458: [D loss: 0.649333, acc: 0.607422]  [A loss: 0.665892, acc: 0.558594]\n",
            "459: [D loss: 0.638931, acc: 0.630859]  [A loss: 1.022807, acc: 0.128906]\n",
            "460: [D loss: 0.658511, acc: 0.601562]  [A loss: 0.725390, acc: 0.468750]\n",
            "461: [D loss: 0.643782, acc: 0.654297]  [A loss: 0.896152, acc: 0.230469]\n",
            "462: [D loss: 0.613096, acc: 0.691406]  [A loss: 0.772208, acc: 0.398438]\n",
            "463: [D loss: 0.628579, acc: 0.646484]  [A loss: 0.940929, acc: 0.175781]\n",
            "464: [D loss: 0.617909, acc: 0.673828]  [A loss: 0.742763, acc: 0.476562]\n",
            "465: [D loss: 0.644616, acc: 0.599609]  [A loss: 0.999393, acc: 0.148438]\n",
            "466: [D loss: 0.618985, acc: 0.656250]  [A loss: 0.727186, acc: 0.500000]\n",
            "467: [D loss: 0.631938, acc: 0.595703]  [A loss: 1.092359, acc: 0.085938]\n",
            "468: [D loss: 0.645164, acc: 0.630859]  [A loss: 0.566807, acc: 0.726562]\n",
            "469: [D loss: 0.660483, acc: 0.593750]  [A loss: 1.213100, acc: 0.031250]\n",
            "470: [D loss: 0.666330, acc: 0.570312]  [A loss: 0.727355, acc: 0.480469]\n",
            "471: [D loss: 0.637180, acc: 0.626953]  [A loss: 0.984645, acc: 0.121094]\n",
            "472: [D loss: 0.637181, acc: 0.630859]  [A loss: 0.748099, acc: 0.468750]\n",
            "473: [D loss: 0.626773, acc: 0.646484]  [A loss: 0.928678, acc: 0.148438]\n",
            "474: [D loss: 0.635781, acc: 0.617188]  [A loss: 0.788652, acc: 0.371094]\n",
            "475: [D loss: 0.623470, acc: 0.652344]  [A loss: 0.896809, acc: 0.214844]\n",
            "476: [D loss: 0.636321, acc: 0.667969]  [A loss: 0.834652, acc: 0.308594]\n",
            "477: [D loss: 0.615709, acc: 0.681641]  [A loss: 0.945880, acc: 0.191406]\n",
            "478: [D loss: 0.629385, acc: 0.673828]  [A loss: 0.704540, acc: 0.515625]\n",
            "479: [D loss: 0.652274, acc: 0.601562]  [A loss: 1.151389, acc: 0.070312]\n",
            "480: [D loss: 0.651670, acc: 0.603516]  [A loss: 0.679423, acc: 0.507812]\n",
            "481: [D loss: 0.650800, acc: 0.609375]  [A loss: 1.117047, acc: 0.078125]\n",
            "482: [D loss: 0.635086, acc: 0.609375]  [A loss: 0.689380, acc: 0.550781]\n",
            "483: [D loss: 0.653154, acc: 0.623047]  [A loss: 1.030046, acc: 0.128906]\n",
            "484: [D loss: 0.652290, acc: 0.605469]  [A loss: 0.755177, acc: 0.414062]\n",
            "485: [D loss: 0.641670, acc: 0.623047]  [A loss: 0.943958, acc: 0.164062]\n",
            "486: [D loss: 0.623229, acc: 0.671875]  [A loss: 0.794622, acc: 0.378906]\n",
            "487: [D loss: 0.619116, acc: 0.636719]  [A loss: 0.938411, acc: 0.171875]\n",
            "488: [D loss: 0.619650, acc: 0.671875]  [A loss: 0.870978, acc: 0.289062]\n",
            "489: [D loss: 0.623596, acc: 0.666016]  [A loss: 0.844788, acc: 0.316406]\n",
            "490: [D loss: 0.638616, acc: 0.642578]  [A loss: 0.913889, acc: 0.226562]\n",
            "491: [D loss: 0.631581, acc: 0.650391]  [A loss: 0.848072, acc: 0.285156]\n",
            "492: [D loss: 0.626109, acc: 0.666016]  [A loss: 1.034996, acc: 0.136719]\n",
            "493: [D loss: 0.636421, acc: 0.646484]  [A loss: 0.601698, acc: 0.671875]\n",
            "494: [D loss: 0.656689, acc: 0.611328]  [A loss: 1.357331, acc: 0.027344]\n",
            "495: [D loss: 0.710731, acc: 0.550781]  [A loss: 0.678358, acc: 0.535156]\n",
            "496: [D loss: 0.665065, acc: 0.585938]  [A loss: 0.941435, acc: 0.179688]\n",
            "497: [D loss: 0.642743, acc: 0.632812]  [A loss: 0.793732, acc: 0.347656]\n",
            "498: [D loss: 0.622638, acc: 0.662109]  [A loss: 0.906215, acc: 0.210938]\n",
            "499: [D loss: 0.636840, acc: 0.640625]  [A loss: 0.823395, acc: 0.351562]\n",
            "500: [D loss: 0.642074, acc: 0.634766]  [A loss: 0.913980, acc: 0.250000]\n",
            "501: [D loss: 0.632406, acc: 0.628906]  [A loss: 0.784584, acc: 0.390625]\n",
            "502: [D loss: 0.645993, acc: 0.648438]  [A loss: 0.938801, acc: 0.210938]\n",
            "503: [D loss: 0.643225, acc: 0.615234]  [A loss: 0.807771, acc: 0.363281]\n",
            "504: [D loss: 0.628061, acc: 0.646484]  [A loss: 0.980568, acc: 0.167969]\n",
            "505: [D loss: 0.646304, acc: 0.630859]  [A loss: 0.746420, acc: 0.433594]\n",
            "506: [D loss: 0.625242, acc: 0.636719]  [A loss: 1.119943, acc: 0.085938]\n",
            "507: [D loss: 0.652497, acc: 0.599609]  [A loss: 0.700755, acc: 0.539062]\n",
            "508: [D loss: 0.664881, acc: 0.593750]  [A loss: 1.037046, acc: 0.105469]\n",
            "509: [D loss: 0.637629, acc: 0.628906]  [A loss: 0.699342, acc: 0.511719]\n",
            "510: [D loss: 0.635897, acc: 0.615234]  [A loss: 0.982169, acc: 0.140625]\n",
            "511: [D loss: 0.627624, acc: 0.646484]  [A loss: 0.745286, acc: 0.460938]\n",
            "512: [D loss: 0.650045, acc: 0.617188]  [A loss: 1.095127, acc: 0.054688]\n",
            "513: [D loss: 0.661684, acc: 0.589844]  [A loss: 0.685727, acc: 0.554688]\n",
            "514: [D loss: 0.629032, acc: 0.642578]  [A loss: 1.005138, acc: 0.156250]\n",
            "515: [D loss: 0.623114, acc: 0.646484]  [A loss: 0.696462, acc: 0.515625]\n",
            "516: [D loss: 0.643302, acc: 0.628906]  [A loss: 1.073920, acc: 0.105469]\n",
            "517: [D loss: 0.658335, acc: 0.597656]  [A loss: 0.721556, acc: 0.468750]\n",
            "518: [D loss: 0.641464, acc: 0.613281]  [A loss: 0.954072, acc: 0.156250]\n",
            "519: [D loss: 0.611310, acc: 0.683594]  [A loss: 0.763513, acc: 0.421875]\n",
            "520: [D loss: 0.648977, acc: 0.630859]  [A loss: 0.963084, acc: 0.160156]\n",
            "521: [D loss: 0.664312, acc: 0.568359]  [A loss: 0.776796, acc: 0.367188]\n",
            "522: [D loss: 0.618565, acc: 0.687500]  [A loss: 0.910448, acc: 0.226562]\n",
            "523: [D loss: 0.647399, acc: 0.617188]  [A loss: 0.881387, acc: 0.238281]\n",
            "524: [D loss: 0.626166, acc: 0.666016]  [A loss: 0.868674, acc: 0.281250]\n",
            "525: [D loss: 0.628946, acc: 0.630859]  [A loss: 0.888466, acc: 0.238281]\n",
            "526: [D loss: 0.616960, acc: 0.662109]  [A loss: 0.837173, acc: 0.296875]\n",
            "527: [D loss: 0.601230, acc: 0.681641]  [A loss: 0.989656, acc: 0.164062]\n",
            "528: [D loss: 0.648067, acc: 0.605469]  [A loss: 0.778330, acc: 0.417969]\n",
            "529: [D loss: 0.636529, acc: 0.623047]  [A loss: 1.169993, acc: 0.105469]\n",
            "530: [D loss: 0.672374, acc: 0.595703]  [A loss: 0.684279, acc: 0.566406]\n",
            "531: [D loss: 0.640622, acc: 0.630859]  [A loss: 1.191618, acc: 0.046875]\n",
            "532: [D loss: 0.674481, acc: 0.589844]  [A loss: 0.652463, acc: 0.613281]\n",
            "533: [D loss: 0.650611, acc: 0.617188]  [A loss: 1.089937, acc: 0.050781]\n",
            "534: [D loss: 0.639205, acc: 0.634766]  [A loss: 0.749747, acc: 0.433594]\n",
            "535: [D loss: 0.668550, acc: 0.580078]  [A loss: 0.936437, acc: 0.199219]\n",
            "536: [D loss: 0.655510, acc: 0.599609]  [A loss: 0.795850, acc: 0.347656]\n",
            "537: [D loss: 0.622434, acc: 0.656250]  [A loss: 0.949030, acc: 0.156250]\n",
            "538: [D loss: 0.616067, acc: 0.693359]  [A loss: 0.824863, acc: 0.296875]\n",
            "539: [D loss: 0.654276, acc: 0.611328]  [A loss: 0.926824, acc: 0.210938]\n",
            "540: [D loss: 0.633318, acc: 0.666016]  [A loss: 0.818030, acc: 0.382812]\n",
            "541: [D loss: 0.642432, acc: 0.599609]  [A loss: 0.930246, acc: 0.199219]\n",
            "542: [D loss: 0.611734, acc: 0.712891]  [A loss: 0.854032, acc: 0.328125]\n",
            "543: [D loss: 0.637114, acc: 0.634766]  [A loss: 0.969317, acc: 0.152344]\n",
            "544: [D loss: 0.641271, acc: 0.621094]  [A loss: 0.809072, acc: 0.359375]\n",
            "545: [D loss: 0.647751, acc: 0.611328]  [A loss: 0.966407, acc: 0.175781]\n",
            "546: [D loss: 0.633529, acc: 0.646484]  [A loss: 0.750665, acc: 0.468750]\n",
            "547: [D loss: 0.669530, acc: 0.580078]  [A loss: 1.124400, acc: 0.058594]\n",
            "548: [D loss: 0.666352, acc: 0.601562]  [A loss: 0.579080, acc: 0.738281]\n",
            "549: [D loss: 0.684050, acc: 0.550781]  [A loss: 1.213900, acc: 0.035156]\n",
            "550: [D loss: 0.701493, acc: 0.580078]  [A loss: 0.738930, acc: 0.457031]\n",
            "551: [D loss: 0.643643, acc: 0.613281]  [A loss: 0.920496, acc: 0.183594]\n",
            "552: [D loss: 0.638250, acc: 0.652344]  [A loss: 0.805133, acc: 0.355469]\n",
            "553: [D loss: 0.626956, acc: 0.660156]  [A loss: 0.868222, acc: 0.253906]\n",
            "554: [D loss: 0.635989, acc: 0.658203]  [A loss: 0.863362, acc: 0.253906]\n",
            "555: [D loss: 0.629853, acc: 0.656250]  [A loss: 0.869491, acc: 0.273438]\n",
            "556: [D loss: 0.638317, acc: 0.638672]  [A loss: 0.878029, acc: 0.257812]\n",
            "557: [D loss: 0.634947, acc: 0.630859]  [A loss: 0.942349, acc: 0.230469]\n",
            "558: [D loss: 0.618347, acc: 0.667969]  [A loss: 0.834654, acc: 0.351562]\n",
            "559: [D loss: 0.630806, acc: 0.652344]  [A loss: 0.967476, acc: 0.175781]\n",
            "560: [D loss: 0.639694, acc: 0.621094]  [A loss: 0.764592, acc: 0.402344]\n",
            "561: [D loss: 0.627331, acc: 0.646484]  [A loss: 1.039340, acc: 0.113281]\n",
            "562: [D loss: 0.639129, acc: 0.615234]  [A loss: 0.722662, acc: 0.523438]\n",
            "563: [D loss: 0.669465, acc: 0.578125]  [A loss: 1.210113, acc: 0.042969]\n",
            "564: [D loss: 0.682367, acc: 0.568359]  [A loss: 0.613403, acc: 0.664062]\n",
            "565: [D loss: 0.644690, acc: 0.615234]  [A loss: 1.065556, acc: 0.113281]\n",
            "566: [D loss: 0.645977, acc: 0.617188]  [A loss: 0.771427, acc: 0.437500]\n",
            "567: [D loss: 0.620115, acc: 0.644531]  [A loss: 0.973930, acc: 0.191406]\n",
            "568: [D loss: 0.621686, acc: 0.666016]  [A loss: 0.817245, acc: 0.347656]\n",
            "569: [D loss: 0.640684, acc: 0.611328]  [A loss: 1.003022, acc: 0.144531]\n",
            "570: [D loss: 0.641380, acc: 0.638672]  [A loss: 0.770588, acc: 0.386719]\n",
            "571: [D loss: 0.648045, acc: 0.609375]  [A loss: 0.976928, acc: 0.156250]\n",
            "572: [D loss: 0.631171, acc: 0.623047]  [A loss: 0.776380, acc: 0.414062]\n",
            "573: [D loss: 0.627246, acc: 0.666016]  [A loss: 1.011568, acc: 0.144531]\n",
            "574: [D loss: 0.659933, acc: 0.638672]  [A loss: 0.774598, acc: 0.453125]\n",
            "575: [D loss: 0.628374, acc: 0.644531]  [A loss: 1.084805, acc: 0.101562]\n",
            "576: [D loss: 0.633133, acc: 0.652344]  [A loss: 0.721224, acc: 0.492188]\n",
            "577: [D loss: 0.633555, acc: 0.640625]  [A loss: 1.093367, acc: 0.097656]\n",
            "578: [D loss: 0.627902, acc: 0.644531]  [A loss: 0.716825, acc: 0.484375]\n",
            "579: [D loss: 0.624946, acc: 0.648438]  [A loss: 1.104018, acc: 0.078125]\n",
            "580: [D loss: 0.673830, acc: 0.597656]  [A loss: 0.667535, acc: 0.550781]\n",
            "581: [D loss: 0.657545, acc: 0.597656]  [A loss: 1.051635, acc: 0.082031]\n",
            "582: [D loss: 0.642906, acc: 0.628906]  [A loss: 0.757560, acc: 0.437500]\n",
            "583: [D loss: 0.641662, acc: 0.644531]  [A loss: 0.908994, acc: 0.203125]\n",
            "584: [D loss: 0.637392, acc: 0.628906]  [A loss: 0.813073, acc: 0.355469]\n",
            "585: [D loss: 0.636566, acc: 0.636719]  [A loss: 0.932899, acc: 0.199219]\n",
            "586: [D loss: 0.620987, acc: 0.662109]  [A loss: 0.803192, acc: 0.359375]\n",
            "587: [D loss: 0.647307, acc: 0.625000]  [A loss: 0.949180, acc: 0.187500]\n",
            "588: [D loss: 0.626607, acc: 0.630859]  [A loss: 0.812951, acc: 0.363281]\n",
            "589: [D loss: 0.632945, acc: 0.650391]  [A loss: 0.959237, acc: 0.214844]\n",
            "590: [D loss: 0.627971, acc: 0.650391]  [A loss: 0.782212, acc: 0.402344]\n",
            "591: [D loss: 0.624267, acc: 0.654297]  [A loss: 0.990873, acc: 0.136719]\n",
            "592: [D loss: 0.653766, acc: 0.615234]  [A loss: 0.679157, acc: 0.550781]\n",
            "593: [D loss: 0.634511, acc: 0.615234]  [A loss: 1.061525, acc: 0.125000]\n",
            "594: [D loss: 0.661868, acc: 0.609375]  [A loss: 0.722129, acc: 0.484375]\n",
            "595: [D loss: 0.655601, acc: 0.601562]  [A loss: 0.996524, acc: 0.128906]\n",
            "596: [D loss: 0.651948, acc: 0.613281]  [A loss: 0.791894, acc: 0.386719]\n",
            "597: [D loss: 0.634597, acc: 0.648438]  [A loss: 0.987868, acc: 0.148438]\n",
            "598: [D loss: 0.617295, acc: 0.671875]  [A loss: 0.741844, acc: 0.484375]\n",
            "599: [D loss: 0.631592, acc: 0.636719]  [A loss: 1.188166, acc: 0.070312]\n",
            "600: [D loss: 0.655489, acc: 0.607422]  [A loss: 0.690585, acc: 0.535156]\n",
            "601: [D loss: 0.638808, acc: 0.611328]  [A loss: 1.085796, acc: 0.074219]\n",
            "602: [D loss: 0.609319, acc: 0.685547]  [A loss: 0.759668, acc: 0.402344]\n",
            "603: [D loss: 0.605693, acc: 0.681641]  [A loss: 0.989301, acc: 0.171875]\n",
            "604: [D loss: 0.641059, acc: 0.632812]  [A loss: 0.844133, acc: 0.328125]\n",
            "605: [D loss: 0.629853, acc: 0.656250]  [A loss: 0.930592, acc: 0.253906]\n",
            "606: [D loss: 0.625458, acc: 0.654297]  [A loss: 0.824892, acc: 0.351562]\n",
            "607: [D loss: 0.660048, acc: 0.595703]  [A loss: 0.887601, acc: 0.242188]\n",
            "608: [D loss: 0.632820, acc: 0.652344]  [A loss: 0.944957, acc: 0.214844]\n",
            "609: [D loss: 0.629249, acc: 0.652344]  [A loss: 0.855579, acc: 0.351562]\n",
            "610: [D loss: 0.631692, acc: 0.626953]  [A loss: 1.048268, acc: 0.144531]\n",
            "611: [D loss: 0.638509, acc: 0.638672]  [A loss: 0.703642, acc: 0.539062]\n",
            "612: [D loss: 0.628757, acc: 0.669922]  [A loss: 1.209600, acc: 0.039062]\n",
            "613: [D loss: 0.667462, acc: 0.578125]  [A loss: 0.680293, acc: 0.531250]\n",
            "614: [D loss: 0.626783, acc: 0.656250]  [A loss: 1.184420, acc: 0.042969]\n",
            "615: [D loss: 0.654239, acc: 0.601562]  [A loss: 0.720256, acc: 0.472656]\n",
            "616: [D loss: 0.643515, acc: 0.625000]  [A loss: 1.034455, acc: 0.101562]\n",
            "617: [D loss: 0.639556, acc: 0.634766]  [A loss: 0.805048, acc: 0.339844]\n",
            "618: [D loss: 0.640997, acc: 0.625000]  [A loss: 0.943928, acc: 0.187500]\n",
            "619: [D loss: 0.609065, acc: 0.681641]  [A loss: 0.849365, acc: 0.328125]\n",
            "620: [D loss: 0.620987, acc: 0.673828]  [A loss: 0.911696, acc: 0.261719]\n",
            "621: [D loss: 0.624305, acc: 0.673828]  [A loss: 0.895604, acc: 0.277344]\n",
            "622: [D loss: 0.636173, acc: 0.628906]  [A loss: 0.877769, acc: 0.269531]\n",
            "623: [D loss: 0.632341, acc: 0.662109]  [A loss: 0.950433, acc: 0.210938]\n",
            "624: [D loss: 0.641537, acc: 0.636719]  [A loss: 0.875621, acc: 0.296875]\n",
            "625: [D loss: 0.631693, acc: 0.658203]  [A loss: 0.918621, acc: 0.230469]\n",
            "626: [D loss: 0.623227, acc: 0.650391]  [A loss: 0.927395, acc: 0.222656]\n",
            "627: [D loss: 0.632045, acc: 0.656250]  [A loss: 0.772087, acc: 0.406250]\n",
            "628: [D loss: 0.639382, acc: 0.628906]  [A loss: 1.110992, acc: 0.082031]\n",
            "629: [D loss: 0.640379, acc: 0.656250]  [A loss: 0.648351, acc: 0.621094]\n",
            "630: [D loss: 0.658840, acc: 0.597656]  [A loss: 1.249816, acc: 0.042969]\n",
            "631: [D loss: 0.663164, acc: 0.564453]  [A loss: 0.754118, acc: 0.410156]\n",
            "632: [D loss: 0.640829, acc: 0.609375]  [A loss: 0.926352, acc: 0.179688]\n",
            "633: [D loss: 0.622684, acc: 0.662109]  [A loss: 0.881718, acc: 0.253906]\n",
            "634: [D loss: 0.632047, acc: 0.652344]  [A loss: 0.909656, acc: 0.199219]\n",
            "635: [D loss: 0.631706, acc: 0.630859]  [A loss: 0.784607, acc: 0.425781]\n",
            "636: [D loss: 0.631632, acc: 0.648438]  [A loss: 0.944727, acc: 0.214844]\n",
            "637: [D loss: 0.613664, acc: 0.677734]  [A loss: 0.844563, acc: 0.335938]\n",
            "638: [D loss: 0.607445, acc: 0.703125]  [A loss: 0.887829, acc: 0.269531]\n",
            "639: [D loss: 0.622080, acc: 0.656250]  [A loss: 0.996082, acc: 0.179688]\n",
            "640: [D loss: 0.617125, acc: 0.666016]  [A loss: 0.853580, acc: 0.328125]\n",
            "641: [D loss: 0.629870, acc: 0.626953]  [A loss: 1.078175, acc: 0.113281]\n",
            "642: [D loss: 0.599691, acc: 0.673828]  [A loss: 0.701566, acc: 0.511719]\n",
            "643: [D loss: 0.627425, acc: 0.630859]  [A loss: 1.177077, acc: 0.062500]\n",
            "644: [D loss: 0.648111, acc: 0.609375]  [A loss: 0.646236, acc: 0.617188]\n",
            "645: [D loss: 0.628420, acc: 0.646484]  [A loss: 1.164429, acc: 0.082031]\n",
            "646: [D loss: 0.652015, acc: 0.625000]  [A loss: 0.776094, acc: 0.414062]\n",
            "647: [D loss: 0.642669, acc: 0.615234]  [A loss: 1.012991, acc: 0.136719]\n",
            "648: [D loss: 0.643735, acc: 0.654297]  [A loss: 0.827066, acc: 0.339844]\n",
            "649: [D loss: 0.635529, acc: 0.623047]  [A loss: 0.982501, acc: 0.171875]\n",
            "650: [D loss: 0.641654, acc: 0.648438]  [A loss: 0.819378, acc: 0.339844]\n",
            "651: [D loss: 0.621921, acc: 0.654297]  [A loss: 0.913106, acc: 0.246094]\n",
            "652: [D loss: 0.620640, acc: 0.664062]  [A loss: 0.895669, acc: 0.265625]\n",
            "653: [D loss: 0.603125, acc: 0.695312]  [A loss: 0.840696, acc: 0.367188]\n",
            "654: [D loss: 0.627554, acc: 0.656250]  [A loss: 1.002570, acc: 0.171875]\n",
            "655: [D loss: 0.623207, acc: 0.675781]  [A loss: 0.798369, acc: 0.363281]\n",
            "656: [D loss: 0.644136, acc: 0.603516]  [A loss: 1.108680, acc: 0.128906]\n",
            "657: [D loss: 0.629802, acc: 0.650391]  [A loss: 0.758683, acc: 0.437500]\n",
            "658: [D loss: 0.655368, acc: 0.607422]  [A loss: 1.115571, acc: 0.101562]\n",
            "659: [D loss: 0.619180, acc: 0.660156]  [A loss: 0.689342, acc: 0.523438]\n",
            "660: [D loss: 0.654954, acc: 0.595703]  [A loss: 1.142948, acc: 0.082031]\n",
            "661: [D loss: 0.624915, acc: 0.648438]  [A loss: 0.811701, acc: 0.359375]\n",
            "662: [D loss: 0.637980, acc: 0.650391]  [A loss: 1.003917, acc: 0.128906]\n",
            "663: [D loss: 0.614595, acc: 0.664062]  [A loss: 0.846305, acc: 0.328125]\n",
            "664: [D loss: 0.612978, acc: 0.664062]  [A loss: 1.000866, acc: 0.160156]\n",
            "665: [D loss: 0.617662, acc: 0.667969]  [A loss: 0.770449, acc: 0.437500]\n",
            "666: [D loss: 0.627396, acc: 0.632812]  [A loss: 1.071913, acc: 0.148438]\n",
            "667: [D loss: 0.636203, acc: 0.636719]  [A loss: 0.767738, acc: 0.449219]\n",
            "668: [D loss: 0.616954, acc: 0.656250]  [A loss: 1.042840, acc: 0.144531]\n",
            "669: [D loss: 0.655555, acc: 0.615234]  [A loss: 0.777564, acc: 0.421875]\n",
            "670: [D loss: 0.635938, acc: 0.644531]  [A loss: 1.092234, acc: 0.132812]\n",
            "671: [D loss: 0.619070, acc: 0.621094]  [A loss: 0.795492, acc: 0.414062]\n",
            "672: [D loss: 0.640613, acc: 0.630859]  [A loss: 0.980659, acc: 0.171875]\n",
            "673: [D loss: 0.633521, acc: 0.666016]  [A loss: 0.879819, acc: 0.273438]\n",
            "674: [D loss: 0.648341, acc: 0.619141]  [A loss: 0.967732, acc: 0.203125]\n",
            "675: [D loss: 0.611304, acc: 0.648438]  [A loss: 0.843535, acc: 0.332031]\n",
            "676: [D loss: 0.637246, acc: 0.630859]  [A loss: 1.015384, acc: 0.132812]\n",
            "677: [D loss: 0.635012, acc: 0.642578]  [A loss: 0.894939, acc: 0.242188]\n",
            "678: [D loss: 0.629638, acc: 0.632812]  [A loss: 0.981148, acc: 0.183594]\n",
            "679: [D loss: 0.637415, acc: 0.638672]  [A loss: 0.816346, acc: 0.367188]\n",
            "680: [D loss: 0.628676, acc: 0.642578]  [A loss: 0.977965, acc: 0.164062]\n",
            "681: [D loss: 0.618941, acc: 0.660156]  [A loss: 0.779050, acc: 0.441406]\n",
            "682: [D loss: 0.649092, acc: 0.621094]  [A loss: 1.187073, acc: 0.070312]\n",
            "683: [D loss: 0.651202, acc: 0.632812]  [A loss: 0.681704, acc: 0.558594]\n",
            "684: [D loss: 0.648368, acc: 0.591797]  [A loss: 1.183491, acc: 0.058594]\n",
            "685: [D loss: 0.646365, acc: 0.630859]  [A loss: 0.822707, acc: 0.363281]\n",
            "686: [D loss: 0.646506, acc: 0.640625]  [A loss: 0.971202, acc: 0.144531]\n",
            "687: [D loss: 0.618434, acc: 0.654297]  [A loss: 0.869146, acc: 0.285156]\n",
            "688: [D loss: 0.619046, acc: 0.648438]  [A loss: 0.842699, acc: 0.335938]\n",
            "689: [D loss: 0.619458, acc: 0.671875]  [A loss: 0.956932, acc: 0.207031]\n",
            "690: [D loss: 0.622033, acc: 0.652344]  [A loss: 0.859029, acc: 0.292969]\n",
            "691: [D loss: 0.626453, acc: 0.642578]  [A loss: 1.092512, acc: 0.105469]\n",
            "692: [D loss: 0.631048, acc: 0.658203]  [A loss: 0.724158, acc: 0.488281]\n",
            "693: [D loss: 0.629211, acc: 0.601562]  [A loss: 1.088754, acc: 0.113281]\n",
            "694: [D loss: 0.618503, acc: 0.667969]  [A loss: 0.759427, acc: 0.410156]\n",
            "695: [D loss: 0.612817, acc: 0.648438]  [A loss: 1.136321, acc: 0.085938]\n",
            "696: [D loss: 0.631666, acc: 0.632812]  [A loss: 0.754011, acc: 0.429688]\n",
            "697: [D loss: 0.621123, acc: 0.654297]  [A loss: 1.083851, acc: 0.093750]\n",
            "698: [D loss: 0.619141, acc: 0.691406]  [A loss: 0.763239, acc: 0.441406]\n",
            "699: [D loss: 0.629556, acc: 0.636719]  [A loss: 1.031096, acc: 0.148438]\n",
            "700: [D loss: 0.626744, acc: 0.650391]  [A loss: 0.817839, acc: 0.355469]\n",
            "701: [D loss: 0.600607, acc: 0.677734]  [A loss: 1.037530, acc: 0.156250]\n",
            "702: [D loss: 0.608871, acc: 0.667969]  [A loss: 0.842443, acc: 0.394531]\n",
            "703: [D loss: 0.620708, acc: 0.658203]  [A loss: 1.082007, acc: 0.132812]\n",
            "704: [D loss: 0.596176, acc: 0.675781]  [A loss: 0.783423, acc: 0.433594]\n",
            "705: [D loss: 0.651916, acc: 0.619141]  [A loss: 1.177347, acc: 0.074219]\n",
            "706: [D loss: 0.635493, acc: 0.619141]  [A loss: 0.679976, acc: 0.554688]\n",
            "707: [D loss: 0.643336, acc: 0.621094]  [A loss: 1.200293, acc: 0.082031]\n",
            "708: [D loss: 0.632332, acc: 0.636719]  [A loss: 0.811932, acc: 0.378906]\n",
            "709: [D loss: 0.642999, acc: 0.630859]  [A loss: 1.031439, acc: 0.160156]\n",
            "710: [D loss: 0.624763, acc: 0.654297]  [A loss: 0.858873, acc: 0.296875]\n",
            "711: [D loss: 0.617482, acc: 0.662109]  [A loss: 0.957247, acc: 0.214844]\n",
            "712: [D loss: 0.624722, acc: 0.638672]  [A loss: 0.887367, acc: 0.300781]\n",
            "713: [D loss: 0.618957, acc: 0.675781]  [A loss: 1.055113, acc: 0.160156]\n",
            "714: [D loss: 0.635102, acc: 0.646484]  [A loss: 0.801588, acc: 0.386719]\n",
            "715: [D loss: 0.639402, acc: 0.628906]  [A loss: 1.084421, acc: 0.136719]\n",
            "716: [D loss: 0.611835, acc: 0.673828]  [A loss: 0.751156, acc: 0.457031]\n",
            "717: [D loss: 0.617993, acc: 0.656250]  [A loss: 1.089610, acc: 0.132812]\n",
            "718: [D loss: 0.624071, acc: 0.621094]  [A loss: 0.734637, acc: 0.476562]\n",
            "719: [D loss: 0.651010, acc: 0.632812]  [A loss: 1.099746, acc: 0.125000]\n",
            "720: [D loss: 0.647268, acc: 0.642578]  [A loss: 0.803129, acc: 0.382812]\n",
            "721: [D loss: 0.609447, acc: 0.666016]  [A loss: 0.978505, acc: 0.203125]\n",
            "722: [D loss: 0.615516, acc: 0.660156]  [A loss: 0.877791, acc: 0.312500]\n",
            "723: [D loss: 0.606153, acc: 0.671875]  [A loss: 1.086136, acc: 0.136719]\n",
            "724: [D loss: 0.625679, acc: 0.666016]  [A loss: 0.784949, acc: 0.382812]\n",
            "725: [D loss: 0.633263, acc: 0.642578]  [A loss: 1.112405, acc: 0.117188]\n",
            "726: [D loss: 0.635903, acc: 0.644531]  [A loss: 0.769721, acc: 0.406250]\n",
            "727: [D loss: 0.630472, acc: 0.650391]  [A loss: 1.110280, acc: 0.093750]\n",
            "728: [D loss: 0.623153, acc: 0.666016]  [A loss: 0.823682, acc: 0.390625]\n",
            "729: [D loss: 0.623024, acc: 0.648438]  [A loss: 1.076613, acc: 0.136719]\n",
            "730: [D loss: 0.646748, acc: 0.628906]  [A loss: 0.844546, acc: 0.332031]\n",
            "731: [D loss: 0.638248, acc: 0.646484]  [A loss: 1.030080, acc: 0.175781]\n",
            "732: [D loss: 0.612983, acc: 0.673828]  [A loss: 0.862466, acc: 0.332031]\n",
            "733: [D loss: 0.637336, acc: 0.625000]  [A loss: 1.120565, acc: 0.101562]\n",
            "734: [D loss: 0.621999, acc: 0.654297]  [A loss: 0.827577, acc: 0.328125]\n",
            "735: [D loss: 0.610494, acc: 0.666016]  [A loss: 1.018695, acc: 0.175781]\n",
            "736: [D loss: 0.631736, acc: 0.644531]  [A loss: 0.848035, acc: 0.324219]\n",
            "737: [D loss: 0.609861, acc: 0.667969]  [A loss: 0.994024, acc: 0.191406]\n",
            "738: [D loss: 0.630551, acc: 0.638672]  [A loss: 0.835370, acc: 0.363281]\n",
            "739: [D loss: 0.642167, acc: 0.626953]  [A loss: 1.065898, acc: 0.140625]\n",
            "740: [D loss: 0.627618, acc: 0.658203]  [A loss: 0.797677, acc: 0.402344]\n",
            "741: [D loss: 0.634068, acc: 0.658203]  [A loss: 1.127521, acc: 0.066406]\n",
            "742: [D loss: 0.628719, acc: 0.656250]  [A loss: 0.763225, acc: 0.468750]\n",
            "743: [D loss: 0.622109, acc: 0.648438]  [A loss: 1.121209, acc: 0.128906]\n",
            "744: [D loss: 0.632213, acc: 0.605469]  [A loss: 0.877746, acc: 0.332031]\n",
            "745: [D loss: 0.609861, acc: 0.691406]  [A loss: 1.003408, acc: 0.226562]\n",
            "746: [D loss: 0.616667, acc: 0.652344]  [A loss: 0.829650, acc: 0.359375]\n",
            "747: [D loss: 0.607959, acc: 0.656250]  [A loss: 1.030293, acc: 0.156250]\n",
            "748: [D loss: 0.615273, acc: 0.642578]  [A loss: 0.878011, acc: 0.289062]\n",
            "749: [D loss: 0.637667, acc: 0.630859]  [A loss: 1.032151, acc: 0.171875]\n",
            "750: [D loss: 0.645078, acc: 0.638672]  [A loss: 0.861458, acc: 0.316406]\n",
            "751: [D loss: 0.604614, acc: 0.685547]  [A loss: 1.037682, acc: 0.164062]\n",
            "752: [D loss: 0.596140, acc: 0.689453]  [A loss: 0.966431, acc: 0.226562]\n",
            "753: [D loss: 0.620494, acc: 0.626953]  [A loss: 1.016130, acc: 0.187500]\n",
            "754: [D loss: 0.617610, acc: 0.666016]  [A loss: 0.812664, acc: 0.421875]\n",
            "755: [D loss: 0.616760, acc: 0.681641]  [A loss: 1.192557, acc: 0.089844]\n",
            "756: [D loss: 0.621228, acc: 0.666016]  [A loss: 0.733746, acc: 0.542969]\n",
            "757: [D loss: 0.664753, acc: 0.570312]  [A loss: 1.344611, acc: 0.050781]\n",
            "758: [D loss: 0.643592, acc: 0.628906]  [A loss: 0.726828, acc: 0.511719]\n",
            "759: [D loss: 0.638257, acc: 0.625000]  [A loss: 1.161083, acc: 0.101562]\n",
            "760: [D loss: 0.640759, acc: 0.640625]  [A loss: 0.842836, acc: 0.355469]\n",
            "761: [D loss: 0.601892, acc: 0.693359]  [A loss: 0.934522, acc: 0.257812]\n",
            "762: [D loss: 0.621259, acc: 0.648438]  [A loss: 0.842765, acc: 0.347656]\n",
            "763: [D loss: 0.627357, acc: 0.630859]  [A loss: 0.978032, acc: 0.179688]\n",
            "764: [D loss: 0.621648, acc: 0.664062]  [A loss: 0.845067, acc: 0.355469]\n",
            "765: [D loss: 0.616893, acc: 0.650391]  [A loss: 1.029613, acc: 0.152344]\n",
            "766: [D loss: 0.616549, acc: 0.664062]  [A loss: 0.920960, acc: 0.250000]\n",
            "767: [D loss: 0.629015, acc: 0.621094]  [A loss: 1.029215, acc: 0.199219]\n",
            "768: [D loss: 0.621825, acc: 0.652344]  [A loss: 0.879873, acc: 0.320312]\n",
            "769: [D loss: 0.603517, acc: 0.679688]  [A loss: 1.110135, acc: 0.125000]\n",
            "770: [D loss: 0.620083, acc: 0.640625]  [A loss: 0.799852, acc: 0.363281]\n",
            "771: [D loss: 0.637624, acc: 0.630859]  [A loss: 1.168040, acc: 0.101562]\n",
            "772: [D loss: 0.618400, acc: 0.662109]  [A loss: 0.746199, acc: 0.480469]\n",
            "773: [D loss: 0.639182, acc: 0.617188]  [A loss: 1.184170, acc: 0.082031]\n",
            "774: [D loss: 0.642085, acc: 0.630859]  [A loss: 0.813151, acc: 0.394531]\n",
            "775: [D loss: 0.638892, acc: 0.636719]  [A loss: 1.065319, acc: 0.136719]\n",
            "776: [D loss: 0.631681, acc: 0.642578]  [A loss: 0.864527, acc: 0.320312]\n",
            "777: [D loss: 0.597973, acc: 0.683594]  [A loss: 0.978855, acc: 0.203125]\n",
            "778: [D loss: 0.624005, acc: 0.646484]  [A loss: 0.932139, acc: 0.300781]\n",
            "779: [D loss: 0.618310, acc: 0.644531]  [A loss: 0.934117, acc: 0.226562]\n",
            "780: [D loss: 0.636779, acc: 0.630859]  [A loss: 0.964036, acc: 0.210938]\n",
            "781: [D loss: 0.598708, acc: 0.699219]  [A loss: 0.904176, acc: 0.261719]\n",
            "782: [D loss: 0.594998, acc: 0.687500]  [A loss: 1.036103, acc: 0.144531]\n",
            "783: [D loss: 0.607469, acc: 0.669922]  [A loss: 0.907041, acc: 0.253906]\n",
            "784: [D loss: 0.619129, acc: 0.660156]  [A loss: 1.117455, acc: 0.093750]\n",
            "785: [D loss: 0.610627, acc: 0.683594]  [A loss: 0.866111, acc: 0.363281]\n",
            "786: [D loss: 0.604405, acc: 0.707031]  [A loss: 1.024065, acc: 0.164062]\n",
            "787: [D loss: 0.612101, acc: 0.677734]  [A loss: 0.905718, acc: 0.328125]\n",
            "788: [D loss: 0.629170, acc: 0.646484]  [A loss: 1.002111, acc: 0.199219]\n",
            "789: [D loss: 0.601954, acc: 0.689453]  [A loss: 0.961357, acc: 0.242188]\n",
            "790: [D loss: 0.619582, acc: 0.638672]  [A loss: 1.018098, acc: 0.207031]\n",
            "791: [D loss: 0.614625, acc: 0.664062]  [A loss: 0.899430, acc: 0.285156]\n",
            "792: [D loss: 0.608305, acc: 0.658203]  [A loss: 1.077508, acc: 0.132812]\n",
            "793: [D loss: 0.629731, acc: 0.640625]  [A loss: 0.767006, acc: 0.453125]\n",
            "794: [D loss: 0.645707, acc: 0.615234]  [A loss: 1.216026, acc: 0.066406]\n",
            "795: [D loss: 0.615933, acc: 0.652344]  [A loss: 0.784613, acc: 0.382812]\n",
            "796: [D loss: 0.642671, acc: 0.638672]  [A loss: 1.288759, acc: 0.035156]\n",
            "797: [D loss: 0.641276, acc: 0.634766]  [A loss: 0.745314, acc: 0.472656]\n",
            "798: [D loss: 0.600194, acc: 0.675781]  [A loss: 1.148446, acc: 0.132812]\n",
            "799: [D loss: 0.641881, acc: 0.623047]  [A loss: 0.811412, acc: 0.335938]\n",
            "800: [D loss: 0.593277, acc: 0.691406]  [A loss: 1.045342, acc: 0.195312]\n",
            "801: [D loss: 0.631878, acc: 0.636719]  [A loss: 0.841404, acc: 0.359375]\n",
            "802: [D loss: 0.613042, acc: 0.660156]  [A loss: 1.003185, acc: 0.191406]\n",
            "803: [D loss: 0.599484, acc: 0.671875]  [A loss: 0.854225, acc: 0.351562]\n",
            "804: [D loss: 0.620304, acc: 0.650391]  [A loss: 1.039531, acc: 0.191406]\n",
            "805: [D loss: 0.643187, acc: 0.617188]  [A loss: 0.871113, acc: 0.351562]\n",
            "806: [D loss: 0.618430, acc: 0.683594]  [A loss: 0.996565, acc: 0.171875]\n",
            "807: [D loss: 0.610599, acc: 0.658203]  [A loss: 0.856532, acc: 0.312500]\n",
            "808: [D loss: 0.626758, acc: 0.636719]  [A loss: 1.085194, acc: 0.121094]\n",
            "809: [D loss: 0.614738, acc: 0.675781]  [A loss: 0.760870, acc: 0.441406]\n",
            "810: [D loss: 0.620724, acc: 0.644531]  [A loss: 1.222509, acc: 0.078125]\n",
            "811: [D loss: 0.649707, acc: 0.611328]  [A loss: 0.804296, acc: 0.386719]\n",
            "812: [D loss: 0.611763, acc: 0.666016]  [A loss: 1.109702, acc: 0.136719]\n",
            "813: [D loss: 0.603932, acc: 0.671875]  [A loss: 0.827816, acc: 0.339844]\n",
            "814: [D loss: 0.635385, acc: 0.652344]  [A loss: 1.006219, acc: 0.175781]\n",
            "815: [D loss: 0.617580, acc: 0.673828]  [A loss: 0.907743, acc: 0.296875]\n",
            "816: [D loss: 0.612556, acc: 0.695312]  [A loss: 0.897084, acc: 0.277344]\n",
            "817: [D loss: 0.612182, acc: 0.652344]  [A loss: 1.009841, acc: 0.191406]\n",
            "818: [D loss: 0.605570, acc: 0.689453]  [A loss: 0.890725, acc: 0.320312]\n",
            "819: [D loss: 0.618020, acc: 0.667969]  [A loss: 1.100656, acc: 0.164062]\n",
            "820: [D loss: 0.629119, acc: 0.644531]  [A loss: 0.816641, acc: 0.359375]\n",
            "821: [D loss: 0.630764, acc: 0.638672]  [A loss: 1.038806, acc: 0.156250]\n",
            "822: [D loss: 0.603036, acc: 0.675781]  [A loss: 0.881332, acc: 0.324219]\n",
            "823: [D loss: 0.608618, acc: 0.695312]  [A loss: 1.091278, acc: 0.136719]\n",
            "824: [D loss: 0.612084, acc: 0.650391]  [A loss: 0.892234, acc: 0.304688]\n",
            "825: [D loss: 0.614859, acc: 0.638672]  [A loss: 1.087239, acc: 0.136719]\n",
            "826: [D loss: 0.621255, acc: 0.652344]  [A loss: 0.855212, acc: 0.351562]\n",
            "827: [D loss: 0.610751, acc: 0.662109]  [A loss: 1.130906, acc: 0.125000]\n",
            "828: [D loss: 0.600232, acc: 0.679688]  [A loss: 0.736589, acc: 0.507812]\n",
            "829: [D loss: 0.620125, acc: 0.666016]  [A loss: 1.175881, acc: 0.113281]\n",
            "830: [D loss: 0.650327, acc: 0.613281]  [A loss: 0.787898, acc: 0.433594]\n",
            "831: [D loss: 0.637102, acc: 0.621094]  [A loss: 1.158530, acc: 0.105469]\n",
            "832: [D loss: 0.622075, acc: 0.644531]  [A loss: 0.832863, acc: 0.367188]\n",
            "833: [D loss: 0.633923, acc: 0.650391]  [A loss: 1.114872, acc: 0.117188]\n",
            "834: [D loss: 0.607413, acc: 0.677734]  [A loss: 0.794921, acc: 0.414062]\n",
            "835: [D loss: 0.628833, acc: 0.652344]  [A loss: 1.153253, acc: 0.082031]\n",
            "836: [D loss: 0.631036, acc: 0.636719]  [A loss: 0.787470, acc: 0.414062]\n",
            "837: [D loss: 0.616337, acc: 0.656250]  [A loss: 1.085286, acc: 0.152344]\n",
            "838: [D loss: 0.632962, acc: 0.658203]  [A loss: 0.868740, acc: 0.296875]\n",
            "839: [D loss: 0.599695, acc: 0.705078]  [A loss: 1.015711, acc: 0.160156]\n",
            "840: [D loss: 0.622199, acc: 0.664062]  [A loss: 1.005265, acc: 0.183594]\n",
            "841: [D loss: 0.596040, acc: 0.673828]  [A loss: 0.950753, acc: 0.222656]\n",
            "842: [D loss: 0.641511, acc: 0.630859]  [A loss: 1.048486, acc: 0.167969]\n",
            "843: [D loss: 0.619397, acc: 0.664062]  [A loss: 0.800654, acc: 0.351562]\n",
            "844: [D loss: 0.619263, acc: 0.656250]  [A loss: 1.056989, acc: 0.156250]\n",
            "845: [D loss: 0.610455, acc: 0.669922]  [A loss: 0.826689, acc: 0.328125]\n",
            "846: [D loss: 0.619690, acc: 0.642578]  [A loss: 1.113839, acc: 0.113281]\n",
            "847: [D loss: 0.606015, acc: 0.669922]  [A loss: 0.863457, acc: 0.343750]\n",
            "848: [D loss: 0.631392, acc: 0.621094]  [A loss: 1.079596, acc: 0.128906]\n",
            "849: [D loss: 0.597599, acc: 0.679688]  [A loss: 0.784638, acc: 0.425781]\n",
            "850: [D loss: 0.650186, acc: 0.626953]  [A loss: 1.259484, acc: 0.093750]\n",
            "851: [D loss: 0.629749, acc: 0.640625]  [A loss: 0.744623, acc: 0.445312]\n",
            "852: [D loss: 0.597891, acc: 0.669922]  [A loss: 1.068521, acc: 0.125000]\n",
            "853: [D loss: 0.601113, acc: 0.681641]  [A loss: 0.855969, acc: 0.347656]\n",
            "854: [D loss: 0.615241, acc: 0.664062]  [A loss: 1.064332, acc: 0.136719]\n",
            "855: [D loss: 0.612431, acc: 0.656250]  [A loss: 0.882522, acc: 0.335938]\n",
            "856: [D loss: 0.639454, acc: 0.636719]  [A loss: 1.067226, acc: 0.125000]\n",
            "857: [D loss: 0.631423, acc: 0.650391]  [A loss: 0.805841, acc: 0.425781]\n",
            "858: [D loss: 0.628029, acc: 0.656250]  [A loss: 1.078342, acc: 0.140625]\n",
            "859: [D loss: 0.594837, acc: 0.687500]  [A loss: 0.890758, acc: 0.277344]\n",
            "860: [D loss: 0.610441, acc: 0.687500]  [A loss: 1.070575, acc: 0.152344]\n",
            "861: [D loss: 0.620889, acc: 0.666016]  [A loss: 0.841139, acc: 0.359375]\n",
            "862: [D loss: 0.619151, acc: 0.658203]  [A loss: 1.076332, acc: 0.164062]\n",
            "863: [D loss: 0.635437, acc: 0.650391]  [A loss: 0.856396, acc: 0.296875]\n",
            "864: [D loss: 0.600970, acc: 0.662109]  [A loss: 1.093026, acc: 0.101562]\n",
            "865: [D loss: 0.621200, acc: 0.671875]  [A loss: 0.891169, acc: 0.281250]\n",
            "866: [D loss: 0.627635, acc: 0.646484]  [A loss: 1.079340, acc: 0.140625]\n",
            "867: [D loss: 0.627097, acc: 0.638672]  [A loss: 0.866557, acc: 0.339844]\n",
            "868: [D loss: 0.608131, acc: 0.689453]  [A loss: 1.124511, acc: 0.117188]\n",
            "869: [D loss: 0.598523, acc: 0.654297]  [A loss: 0.886222, acc: 0.296875]\n",
            "870: [D loss: 0.608862, acc: 0.683594]  [A loss: 1.098418, acc: 0.097656]\n",
            "871: [D loss: 0.598008, acc: 0.683594]  [A loss: 0.899784, acc: 0.292969]\n",
            "872: [D loss: 0.619408, acc: 0.640625]  [A loss: 1.045211, acc: 0.152344]\n",
            "873: [D loss: 0.626005, acc: 0.667969]  [A loss: 0.831625, acc: 0.343750]\n",
            "874: [D loss: 0.624769, acc: 0.654297]  [A loss: 1.169038, acc: 0.125000]\n",
            "875: [D loss: 0.592465, acc: 0.675781]  [A loss: 0.784020, acc: 0.402344]\n",
            "876: [D loss: 0.588698, acc: 0.675781]  [A loss: 1.276261, acc: 0.074219]\n",
            "877: [D loss: 0.623322, acc: 0.650391]  [A loss: 0.758296, acc: 0.425781]\n",
            "878: [D loss: 0.655193, acc: 0.644531]  [A loss: 1.279084, acc: 0.078125]\n",
            "879: [D loss: 0.633172, acc: 0.644531]  [A loss: 0.867294, acc: 0.378906]\n",
            "880: [D loss: 0.631145, acc: 0.660156]  [A loss: 1.070195, acc: 0.156250]\n",
            "881: [D loss: 0.615711, acc: 0.658203]  [A loss: 0.929112, acc: 0.246094]\n",
            "882: [D loss: 0.592012, acc: 0.693359]  [A loss: 1.022659, acc: 0.164062]\n",
            "883: [D loss: 0.606669, acc: 0.693359]  [A loss: 0.907261, acc: 0.292969]\n",
            "884: [D loss: 0.622668, acc: 0.664062]  [A loss: 0.973331, acc: 0.242188]\n",
            "885: [D loss: 0.613251, acc: 0.652344]  [A loss: 0.898458, acc: 0.285156]\n",
            "886: [D loss: 0.602310, acc: 0.685547]  [A loss: 0.964148, acc: 0.214844]\n",
            "887: [D loss: 0.611720, acc: 0.660156]  [A loss: 0.975594, acc: 0.203125]\n",
            "888: [D loss: 0.604212, acc: 0.671875]  [A loss: 0.951314, acc: 0.210938]\n",
            "889: [D loss: 0.592516, acc: 0.705078]  [A loss: 1.049452, acc: 0.187500]\n",
            "890: [D loss: 0.617461, acc: 0.677734]  [A loss: 0.916070, acc: 0.332031]\n",
            "891: [D loss: 0.635114, acc: 0.632812]  [A loss: 1.185687, acc: 0.093750]\n",
            "892: [D loss: 0.607835, acc: 0.683594]  [A loss: 0.813187, acc: 0.382812]\n",
            "893: [D loss: 0.611471, acc: 0.675781]  [A loss: 1.231499, acc: 0.101562]\n",
            "894: [D loss: 0.617016, acc: 0.632812]  [A loss: 0.794435, acc: 0.433594]\n",
            "895: [D loss: 0.629893, acc: 0.642578]  [A loss: 1.240566, acc: 0.113281]\n",
            "896: [D loss: 0.657116, acc: 0.601562]  [A loss: 0.760223, acc: 0.417969]\n",
            "897: [D loss: 0.616021, acc: 0.660156]  [A loss: 1.094976, acc: 0.140625]\n",
            "898: [D loss: 0.609986, acc: 0.669922]  [A loss: 0.867357, acc: 0.300781]\n",
            "899: [D loss: 0.626031, acc: 0.660156]  [A loss: 1.093868, acc: 0.136719]\n",
            "900: [D loss: 0.603652, acc: 0.691406]  [A loss: 0.845554, acc: 0.343750]\n",
            "901: [D loss: 0.602644, acc: 0.683594]  [A loss: 1.169840, acc: 0.109375]\n",
            "902: [D loss: 0.632912, acc: 0.644531]  [A loss: 0.841967, acc: 0.343750]\n",
            "903: [D loss: 0.610470, acc: 0.669922]  [A loss: 1.049952, acc: 0.164062]\n",
            "904: [D loss: 0.601100, acc: 0.685547]  [A loss: 0.823482, acc: 0.382812]\n",
            "905: [D loss: 0.612208, acc: 0.685547]  [A loss: 1.142064, acc: 0.113281]\n",
            "906: [D loss: 0.625372, acc: 0.660156]  [A loss: 0.845625, acc: 0.351562]\n",
            "907: [D loss: 0.603366, acc: 0.656250]  [A loss: 1.121952, acc: 0.140625]\n",
            "908: [D loss: 0.600635, acc: 0.677734]  [A loss: 0.786699, acc: 0.410156]\n",
            "909: [D loss: 0.623773, acc: 0.662109]  [A loss: 1.179328, acc: 0.117188]\n",
            "910: [D loss: 0.635662, acc: 0.621094]  [A loss: 0.812686, acc: 0.417969]\n",
            "911: [D loss: 0.619287, acc: 0.652344]  [A loss: 1.132173, acc: 0.105469]\n",
            "912: [D loss: 0.599589, acc: 0.673828]  [A loss: 0.847879, acc: 0.304688]\n",
            "913: [D loss: 0.582501, acc: 0.705078]  [A loss: 1.033670, acc: 0.156250]\n",
            "914: [D loss: 0.588586, acc: 0.687500]  [A loss: 0.956182, acc: 0.226562]\n",
            "915: [D loss: 0.608586, acc: 0.679688]  [A loss: 0.994812, acc: 0.226562]\n",
            "916: [D loss: 0.594302, acc: 0.667969]  [A loss: 1.025486, acc: 0.203125]\n",
            "917: [D loss: 0.600320, acc: 0.699219]  [A loss: 0.981652, acc: 0.230469]\n",
            "918: [D loss: 0.587512, acc: 0.660156]  [A loss: 0.958104, acc: 0.222656]\n",
            "919: [D loss: 0.595889, acc: 0.695312]  [A loss: 1.125987, acc: 0.152344]\n",
            "920: [D loss: 0.610916, acc: 0.667969]  [A loss: 0.811407, acc: 0.406250]\n",
            "921: [D loss: 0.611632, acc: 0.650391]  [A loss: 1.180582, acc: 0.128906]\n",
            "922: [D loss: 0.612592, acc: 0.652344]  [A loss: 0.805489, acc: 0.406250]\n",
            "923: [D loss: 0.634521, acc: 0.634766]  [A loss: 1.220148, acc: 0.085938]\n",
            "924: [D loss: 0.620140, acc: 0.658203]  [A loss: 0.823606, acc: 0.375000]\n",
            "925: [D loss: 0.630708, acc: 0.632812]  [A loss: 1.113383, acc: 0.117188]\n",
            "926: [D loss: 0.617404, acc: 0.671875]  [A loss: 0.850318, acc: 0.359375]\n",
            "927: [D loss: 0.639918, acc: 0.664062]  [A loss: 1.082529, acc: 0.160156]\n",
            "928: [D loss: 0.633400, acc: 0.628906]  [A loss: 0.870685, acc: 0.328125]\n",
            "929: [D loss: 0.627547, acc: 0.634766]  [A loss: 1.030380, acc: 0.179688]\n",
            "930: [D loss: 0.577792, acc: 0.712891]  [A loss: 0.954292, acc: 0.203125]\n",
            "931: [D loss: 0.614143, acc: 0.650391]  [A loss: 1.090430, acc: 0.152344]\n",
            "932: [D loss: 0.606326, acc: 0.666016]  [A loss: 0.952454, acc: 0.265625]\n",
            "933: [D loss: 0.629429, acc: 0.669922]  [A loss: 1.090778, acc: 0.160156]\n",
            "934: [D loss: 0.609506, acc: 0.673828]  [A loss: 0.929892, acc: 0.261719]\n",
            "935: [D loss: 0.618519, acc: 0.662109]  [A loss: 1.091789, acc: 0.175781]\n",
            "936: [D loss: 0.622929, acc: 0.642578]  [A loss: 0.852483, acc: 0.347656]\n",
            "937: [D loss: 0.593321, acc: 0.664062]  [A loss: 1.122149, acc: 0.125000]\n",
            "938: [D loss: 0.598345, acc: 0.695312]  [A loss: 0.847409, acc: 0.367188]\n",
            "939: [D loss: 0.592492, acc: 0.681641]  [A loss: 1.205296, acc: 0.089844]\n",
            "940: [D loss: 0.624968, acc: 0.646484]  [A loss: 0.776035, acc: 0.484375]\n",
            "941: [D loss: 0.631024, acc: 0.656250]  [A loss: 1.245281, acc: 0.101562]\n",
            "942: [D loss: 0.649917, acc: 0.603516]  [A loss: 0.747856, acc: 0.511719]\n",
            "943: [D loss: 0.648513, acc: 0.634766]  [A loss: 1.168754, acc: 0.089844]\n",
            "944: [D loss: 0.605080, acc: 0.658203]  [A loss: 0.841761, acc: 0.367188]\n",
            "945: [D loss: 0.590459, acc: 0.683594]  [A loss: 1.096450, acc: 0.132812]\n",
            "946: [D loss: 0.590269, acc: 0.703125]  [A loss: 0.953801, acc: 0.230469]\n",
            "947: [D loss: 0.604972, acc: 0.644531]  [A loss: 0.970897, acc: 0.242188]\n",
            "948: [D loss: 0.614856, acc: 0.675781]  [A loss: 0.935374, acc: 0.281250]\n",
            "949: [D loss: 0.586832, acc: 0.689453]  [A loss: 1.022634, acc: 0.226562]\n",
            "950: [D loss: 0.616820, acc: 0.660156]  [A loss: 0.907432, acc: 0.292969]\n",
            "951: [D loss: 0.583309, acc: 0.708984]  [A loss: 1.062727, acc: 0.167969]\n",
            "952: [D loss: 0.614558, acc: 0.689453]  [A loss: 0.919956, acc: 0.277344]\n",
            "953: [D loss: 0.603514, acc: 0.679688]  [A loss: 1.125010, acc: 0.152344]\n",
            "954: [D loss: 0.598723, acc: 0.675781]  [A loss: 0.793310, acc: 0.382812]\n",
            "955: [D loss: 0.615320, acc: 0.654297]  [A loss: 1.281970, acc: 0.066406]\n",
            "956: [D loss: 0.602221, acc: 0.658203]  [A loss: 0.787647, acc: 0.406250]\n",
            "957: [D loss: 0.610519, acc: 0.654297]  [A loss: 1.250879, acc: 0.089844]\n",
            "958: [D loss: 0.642253, acc: 0.619141]  [A loss: 0.781169, acc: 0.414062]\n",
            "959: [D loss: 0.592025, acc: 0.666016]  [A loss: 1.109637, acc: 0.140625]\n",
            "960: [D loss: 0.602946, acc: 0.681641]  [A loss: 0.933922, acc: 0.265625]\n",
            "961: [D loss: 0.613629, acc: 0.667969]  [A loss: 1.229521, acc: 0.125000]\n",
            "962: [D loss: 0.633844, acc: 0.642578]  [A loss: 0.841730, acc: 0.363281]\n",
            "963: [D loss: 0.601492, acc: 0.679688]  [A loss: 1.125757, acc: 0.117188]\n",
            "964: [D loss: 0.617409, acc: 0.658203]  [A loss: 0.902076, acc: 0.292969]\n",
            "965: [D loss: 0.601324, acc: 0.667969]  [A loss: 0.954482, acc: 0.238281]\n",
            "966: [D loss: 0.581954, acc: 0.689453]  [A loss: 1.037098, acc: 0.207031]\n",
            "967: [D loss: 0.606219, acc: 0.691406]  [A loss: 1.052490, acc: 0.183594]\n",
            "968: [D loss: 0.595983, acc: 0.693359]  [A loss: 0.948324, acc: 0.281250]\n",
            "969: [D loss: 0.579575, acc: 0.662109]  [A loss: 1.092490, acc: 0.175781]\n",
            "970: [D loss: 0.603131, acc: 0.683594]  [A loss: 0.914433, acc: 0.308594]\n",
            "971: [D loss: 0.587841, acc: 0.685547]  [A loss: 1.194548, acc: 0.128906]\n",
            "972: [D loss: 0.620351, acc: 0.644531]  [A loss: 0.853429, acc: 0.359375]\n",
            "973: [D loss: 0.592242, acc: 0.664062]  [A loss: 1.332931, acc: 0.066406]\n",
            "974: [D loss: 0.617319, acc: 0.656250]  [A loss: 0.756911, acc: 0.496094]\n",
            "975: [D loss: 0.626585, acc: 0.650391]  [A loss: 1.216058, acc: 0.089844]\n",
            "976: [D loss: 0.594261, acc: 0.685547]  [A loss: 0.901893, acc: 0.335938]\n",
            "977: [D loss: 0.608621, acc: 0.652344]  [A loss: 1.125174, acc: 0.152344]\n",
            "978: [D loss: 0.595466, acc: 0.681641]  [A loss: 0.915031, acc: 0.269531]\n",
            "979: [D loss: 0.577959, acc: 0.722656]  [A loss: 1.140567, acc: 0.136719]\n",
            "980: [D loss: 0.558597, acc: 0.732422]  [A loss: 1.055133, acc: 0.230469]\n",
            "981: [D loss: 0.630012, acc: 0.638672]  [A loss: 1.115904, acc: 0.191406]\n",
            "982: [D loss: 0.601856, acc: 0.699219]  [A loss: 0.884065, acc: 0.351562]\n",
            "983: [D loss: 0.626996, acc: 0.650391]  [A loss: 1.188591, acc: 0.140625]\n",
            "984: [D loss: 0.615079, acc: 0.654297]  [A loss: 0.810152, acc: 0.382812]\n",
            "985: [D loss: 0.580700, acc: 0.701172]  [A loss: 1.157371, acc: 0.117188]\n",
            "986: [D loss: 0.629619, acc: 0.654297]  [A loss: 0.924198, acc: 0.304688]\n",
            "987: [D loss: 0.628012, acc: 0.638672]  [A loss: 1.139325, acc: 0.144531]\n",
            "988: [D loss: 0.613980, acc: 0.656250]  [A loss: 0.856017, acc: 0.406250]\n",
            "989: [D loss: 0.593690, acc: 0.683594]  [A loss: 1.051029, acc: 0.187500]\n",
            "990: [D loss: 0.595038, acc: 0.689453]  [A loss: 1.039430, acc: 0.179688]\n",
            "991: [D loss: 0.596340, acc: 0.679688]  [A loss: 1.054057, acc: 0.160156]\n",
            "992: [D loss: 0.616447, acc: 0.683594]  [A loss: 0.914417, acc: 0.285156]\n",
            "993: [D loss: 0.622175, acc: 0.662109]  [A loss: 1.040040, acc: 0.195312]\n",
            "994: [D loss: 0.590365, acc: 0.689453]  [A loss: 0.902845, acc: 0.296875]\n",
            "995: [D loss: 0.609791, acc: 0.685547]  [A loss: 1.145468, acc: 0.121094]\n",
            "996: [D loss: 0.621069, acc: 0.660156]  [A loss: 0.919190, acc: 0.257812]\n",
            "997: [D loss: 0.612970, acc: 0.640625]  [A loss: 1.077577, acc: 0.199219]\n",
            "998: [D loss: 0.580516, acc: 0.695312]  [A loss: 0.841859, acc: 0.339844]\n",
            "999: [D loss: 0.570735, acc: 0.714844]  [A loss: 1.127458, acc: 0.152344]\n",
            "Elapsed: 3.866619110107422 min \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}